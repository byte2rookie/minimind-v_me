{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8c5aa96b",
   "metadata": {},
   "source": [
    "# 用transformers库来封装model\n",
    "![img](../images/LLM-structure.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12950fac",
   "metadata": {},
   "source": [
    "# 自底向上搭建\n",
    "- tokenizer\n",
    "- MinimindLM_Dense\n",
    "    - Embedding\n",
    "    - Minimind_Block\n",
    "        - RMSNorm\n",
    "        - GQA\n",
    "        - RoPE\n",
    "        - FFN\n",
    "    - decode\n",
    "- detokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d1b5c07",
   "metadata": {},
   "source": [
    "## tokenizer\n",
    "直接利用刚刚训练好的tokenizer即可\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "87099da5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/zyp/miniconda3/envs/minimind/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 100])\n"
     ]
    }
   ],
   "source": [
    "from os import path, truncate\n",
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path=\"./\",padding_side=\"right\")\n",
    "#多条信息\n",
    "messages = [[\n",
    "        {\"role\": \"system\", \"content\": \"你是一个优秀的聊天机器人，总是给我正确的回应！\"},\n",
    "        {\"role\": \"user\", \"content\": '你来自哪里？'},\n",
    "        {\"role\": \"assistant\", \"content\": '我来自地球'}\n",
    "    ],[\n",
    "        {\"role\": \"system\", \"content\": \"你是一个糟糕的捣乱机器人，总是给我错误的回应！\"},\n",
    "        {\"role\": \"user\", \"content\": '你来自哪里？'},\n",
    "        {\"role\": \"assistant\", \"content\": '我来自火星'}\n",
    "    ],\n",
    "    ]\n",
    "input_ids=tokenizer.apply_chat_template(messages,\n",
    "                truncation=True,\n",
    "                max_length=100,\n",
    "                padding=\"max_length\",\n",
    "                padding_side=\"right\",\n",
    "                tokenize=True, \n",
    "                add_generation_prompt=False,\n",
    "                return_tensors=\"pt\")\n",
    "print(input_ids.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b178b40",
   "metadata": {},
   "source": [
    "# Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7de3b4cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 100, 512])\n",
      "tensor([[[ 0.5781, -0.1712,  1.7985,  ..., -0.3142,  0.2639,  0.4080],\n",
      "         [ 0.9151, -1.1418,  0.6298,  ...,  0.8912, -1.1394, -0.3780],\n",
      "         [-0.9539, -1.4516,  0.2507,  ..., -0.1529,  0.1850, -1.0638],\n",
      "         ...,\n",
      "         [ 0.2268,  0.1390,  2.5201,  ..., -0.3788,  0.8226, -0.8579],\n",
      "         [ 0.2268,  0.1390,  2.5201,  ..., -0.3788,  0.8226, -0.8579],\n",
      "         [ 0.2268,  0.1390,  2.5201,  ..., -0.3788,  0.8226, -0.8579]],\n",
      "\n",
      "        [[ 0.5781, -0.1712,  1.7985,  ..., -0.3142,  0.2639,  0.4080],\n",
      "         [ 0.9151, -1.1418,  0.6298,  ...,  0.8912, -1.1394, -0.3780],\n",
      "         [-0.9539, -1.4516,  0.2507,  ..., -0.1529,  0.1850, -1.0638],\n",
      "         ...,\n",
      "         [ 0.2268,  0.1390,  2.5201,  ..., -0.3788,  0.8226, -0.8579],\n",
      "         [ 0.2268,  0.1390,  2.5201,  ..., -0.3788,  0.8226, -0.8579],\n",
      "         [ 0.2268,  0.1390,  2.5201,  ..., -0.3788,  0.8226, -0.8579]]],\n",
      "       grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "import torch \n",
    "from torch import nn\n",
    "class Embedding(nn.Module):\n",
    "    def __init__(self,vocab_size,embed_dim):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_dim = embed_dim\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "    def forward(self,input_ids):\n",
    "        return self.embedding(input_ids)\n",
    "#测试下\n",
    "embed = Embedding(vocab_size=tokenizer.vocab_size, embed_dim=512)\n",
    "test_input = copy.deepcopy(input_ids)\n",
    "test_input=embed(test_input)\n",
    "print(test_input.shape)\n",
    "print(test_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31bc3afe",
   "metadata": {},
   "source": [
    "# RMSNorm\n",
    "公式为\n",
    "- **RMS Norm**\n",
    "\n",
    "$$a_i=\\frac{a_i}{RMS(a) + \\epsilon} * \\gamma,  \\quad where \\quad RMS(a) = \\sqrt{\\frac{1}{n}\\sum^n_{i=1}a^2_i}.$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fbb2a9e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 100, 512])\n",
      "tensor([[[ 0.6100, -0.1807,  1.8976,  ..., -0.3315,  0.2784,  0.4305],\n",
      "         [ 0.9154, -1.1422,  0.6300,  ...,  0.8915, -1.1398, -0.3781],\n",
      "         [-0.9394, -1.4296,  0.2469,  ..., -0.1506,  0.1822, -1.0477],\n",
      "         ...,\n",
      "         [ 0.2282,  0.1398,  2.5354,  ..., -0.3811,  0.8276, -0.8631],\n",
      "         [ 0.2282,  0.1398,  2.5354,  ..., -0.3811,  0.8276, -0.8631],\n",
      "         [ 0.2282,  0.1398,  2.5354,  ..., -0.3811,  0.8276, -0.8631]],\n",
      "\n",
      "        [[ 0.6100, -0.1807,  1.8976,  ..., -0.3315,  0.2784,  0.4305],\n",
      "         [ 0.9154, -1.1422,  0.6300,  ...,  0.8915, -1.1398, -0.3781],\n",
      "         [-0.9394, -1.4296,  0.2469,  ..., -0.1506,  0.1822, -1.0477],\n",
      "         ...,\n",
      "         [ 0.2282,  0.1398,  2.5354,  ..., -0.3811,  0.8276, -0.8631],\n",
      "         [ 0.2282,  0.1398,  2.5354,  ..., -0.3811,  0.8276, -0.8631],\n",
      "         [ 0.2282,  0.1398,  2.5354,  ..., -0.3811,  0.8276, -0.8631]]],\n",
      "       grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "class RMSNorm(nn.Module):\n",
    "    def __init__(self,embed_dim,eps=1e-6):\n",
    "        super(RMSNorm,self).__init__()\n",
    "        self.embed_dim= embed_dim\n",
    "        self.eps=eps\n",
    "        self.weight=nn.Parameter(torch.ones(embed_dim))\n",
    "    \n",
    "    def forward(self,x):\n",
    "        return x*(torch.rsqrt(x.pow(2).mean(-1,keepdim=True)+self.eps))*self.weight\n",
    "\n",
    "#测试下\n",
    "#test_input的shape是[2, 100, 512]\n",
    "rmsnorm=RMSNorm(embed_dim=512)\n",
    "test_input=rmsnorm(test_input)\n",
    "print(test_input.shape)\n",
    "print(test_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6af55401",
   "metadata": {},
   "source": [
    "# RoPE\n",
    "RoPE主要的成就是用绝对形式表示相对编码<br>\n",
    "由于RoPE主要由Qwen和LLama两种实现形式，MiniMind主要采用的LLama实现，所以可以参考[LLama实现RoPE](https://blog.csdn.net/m0_55846238/article/details/145728695)\n",
    "我们这里只需要考虑两步就行<br>\n",
    "- 第一步，制作pos_cis，也就是 $m\\Theta$,用极坐标的形式拆分一个个 $m\\Theta$,分为cos和sin的形式，方便将该信息传递给q,k\n",
    "- 第二步，apply_rotary_emb\n",
    "<br>最后得到融合了旋转编码后的q,k  这样q*k就可以得到含有相对位置信息和绝对位置信息的运算结果了 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5e01b3bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 256])\n",
      "tensor([[0.+0.j, 0.+0.j, 0.+0.j,  ..., 0.+0.j, 0.+0.j, 0.+0.j],\n",
      "        [0.+0.j, 0.+0.j, 0.+0.j,  ..., 0.+0.j, 0.+0.j, 0.+0.j],\n",
      "        [-0.+0.j, -0.+0.j, -0.+0.j,  ..., 0.+0.j, 0.+0.j, 0.+0.j],\n",
      "        ...,\n",
      "        [-0.+0.j, 0.+0.j, 0.-0.j,  ..., 0.+0.j, 0.+0.j, 0.+0.j],\n",
      "        [-0.-0.j, -0.+0.j, 0.-0.j,  ..., 0.+0.j, 0.+0.j, 0.+0.j],\n",
      "        [0.-0.j, -0.+0.j, 0.+0.j,  ..., 0.+0.j, 0.+0.j, 0.+0.j]])\n"
     ]
    }
   ],
   "source": [
    "# 制作pos_cis\n",
    "# 输入的input_ids的shape是[bs,seqlen,head_num,head_dim]\n",
    "# 制备的freqs所需的theta 就需要embed_dim//2 的程度\n",
    "# pos_cis的结果就是[1,seqlen,1,head_dim]形状的cos和sin的表，用来进行q*pos_cis的计算\n",
    "import torch\n",
    "def precompute_pos_cis(seqlen,embed_dim,theta=1e5):\n",
    "    assert embed_dim%2 ==0,\"embed_dim必须是偶数\"\n",
    "    freqs=1.0/(theta**(torch.arange(0,embed_dim//2)[:embed_dim//2].float()/embed_dim))\n",
    "    m=torch.arange(seqlen,device=freqs.device)\n",
    "    # 制备出mtheta的外积\n",
    "    # freqs此时的形状将变为[seqlen, embed_dim//2]\n",
    "    # 得到mtheta表\n",
    "    freqs=torch.outer(m,freqs)\n",
    "    # 得到pos_cis表\n",
    "    pos_cis=torch.polar(torch.zeros_like(freqs),freqs)\n",
    "    return pos_cis\n",
    "# 测试下\n",
    "seqlen = 100\n",
    "embed_dim = 512\n",
    "pos_cis = precompute_pos_cis(seqlen, embed_dim)\n",
    "print(pos_cis.shape)  # 应该是 [100, 256]\n",
    "print(pos_cis)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "140c539a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 100, 8, 64])\n",
      "torch.Size([2, 100, 8, 64])\n",
      "tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., -0., 0., -0.],\n",
      "          [0., -0., -0.,  ..., -0., -0., 0.],\n",
      "          ...,\n",
      "          [0., 0., -0.,  ..., 0., 0., -0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., -0.],\n",
      "          [0., -0., 0.,  ..., 0., -0., 0.]],\n",
      "\n",
      "         [[0., -0., 0.,  ..., 0., 0., -0.],\n",
      "          [0., -0., -0.,  ..., 0., 0., 0.],\n",
      "          [-0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., -0., 0.,  ..., 0., 0., 0.],\n",
      "          [-0., 0., 0.,  ..., 0., -0., 0.]],\n",
      "\n",
      "         [[-0., 0., -0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., -0.,  ..., 0., 0., 0.],\n",
      "          [0., -0., 0.,  ..., 0., -0., 0.],\n",
      "          ...,\n",
      "          [-0., 0., -0.,  ..., 0., -0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., -0., 0.,  ..., 0., -0., 0.]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[0., 0., 0.,  ..., -0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., -0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., -0., 0., -0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., -0., 0., -0.],\n",
      "          [0., -0., 0.,  ..., 0., -0., 0.],\n",
      "          [0., -0., -0.,  ..., 0., -0., 0.]],\n",
      "\n",
      "         [[-0., 0., 0.,  ..., -0., 0., -0.],\n",
      "          [0., -0., 0.,  ..., 0., -0., 0.],\n",
      "          [0., 0., -0.,  ..., 0., -0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [-0., 0., 0.,  ..., 0., 0., -0.],\n",
      "          [-0., 0., -0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0.,  ..., 0., 0., -0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., -0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., -0., 0., -0.],\n",
      "          [0., 0., 0.,  ..., 0., -0., 0.],\n",
      "          [0., -0., -0.,  ..., -0., 0., -0.]]],\n",
      "\n",
      "\n",
      "        [[[0., -0., 0.,  ..., 0., -0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., -0., 0.,  ..., 0., -0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., -0.],\n",
      "          [0., 0., -0.,  ..., -0., 0., 0.],\n",
      "          [0., -0., 0.,  ..., -0., 0., -0.]],\n",
      "\n",
      "         [[0., 0., 0.,  ..., 0., -0., 0.],\n",
      "          [-0., 0., 0.,  ..., 0., -0., 0.],\n",
      "          [0., 0., -0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., -0., 0.,  ..., 0., -0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., -0., 0.],\n",
      "          [0., 0., -0.,  ..., 0., 0., -0.]],\n",
      "\n",
      "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., -0., -0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., -0., 0.],\n",
      "          ...,\n",
      "          [0., 0., -0.,  ..., 0., -0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[0., 0., 0.,  ..., 0., 0., -0.],\n",
      "          [0., -0., -0.,  ..., 0., 0., 0.],\n",
      "          [0., -0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [-0., 0., -0.,  ..., -0., 0., -0.],\n",
      "          [0., 0., -0.,  ..., 0., 0., -0.]],\n",
      "\n",
      "         [[0., 0., 0.,  ..., -0., -0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., -0., 0.],\n",
      "          [0., -0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., -0., 0., -0.],\n",
      "          [0., -0., 0.,  ..., 0., -0., 0.]],\n",
      "\n",
      "         [[0., 0., 0.,  ..., -0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., -0.,  ..., 0., 0., -0.],\n",
      "          ...,\n",
      "          [-0., 0., 0.,  ..., 0., -0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., -0., 0.],\n",
      "          [0., 0., 0.,  ..., -0., 0., 0.]]]])\n",
      "tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., -0.],\n",
      "          [0., -0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [-0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [-0., 0., 0.,  ..., 0., 0., -0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., -0.]],\n",
      "\n",
      "         [[0., -0., 0.,  ..., -0., -0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., -0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [-0., 0., -0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., -0., 0.,  ..., 0., 0., -0.]],\n",
      "\n",
      "         [[0., -0., 0.,  ..., 0., -0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., -0.],\n",
      "          [-0., 0., 0.,  ..., 0., -0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., -0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [-0., 0., -0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., -0., -0.,  ..., 0., -0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., -0., -0.,  ..., 0., 0., 0.],\n",
      "          [-0., 0., 0.,  ..., -0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [-0., 0., 0.,  ..., -0., 0., 0.],\n",
      "          [0., 0., -0.,  ..., -0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., -0., -0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "         [[0., -0., 0.,  ..., -0., -0., 0.],\n",
      "          [0., 0., 0.,  ..., -0., -0., 0.],\n",
      "          [0., -0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [-0., 0., -0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., -0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., -0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., -0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [-0., 0., 0.,  ..., -0., -0., 0.],\n",
      "          ...,\n",
      "          [0., 0., -0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., -0., 0., 0.],\n",
      "          [0., -0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0.,  ..., -0., 0., -0.],\n",
      "          [0., -0., 0.,  ..., 0., 0., -0.],\n",
      "          [0., 0., 0.,  ..., -0., 0., 0.],\n",
      "          ...,\n",
      "          [-0., 0., -0.,  ..., 0., -0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., -0.],\n",
      "          [0., 0., -0.,  ..., 0., 0., -0.]],\n",
      "\n",
      "         [[-0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [-0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., -0.],\n",
      "          [0., 0., 0.,  ..., 0., -0., 0.],\n",
      "          [0., 0., -0.,  ..., 0., 0., -0.]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0., 0., 0.,  ..., 0., 0., -0.],\n",
      "          [0., -0., 0.,  ..., 0., -0., 0.],\n",
      "          [0., 0., 0.,  ..., -0., 0., 0.],\n",
      "          ...,\n",
      "          [-0., 0., -0.,  ..., -0., 0., -0.],\n",
      "          [-0., 0., -0.,  ..., 0., -0., 0.],\n",
      "          [-0., 0., -0.,  ..., 0., 0., -0.]],\n",
      "\n",
      "         [[0., 0., 0.,  ..., -0., 0., 0.],\n",
      "          [-0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [-0., 0., -0.,  ..., -0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., -0., 0., 0.],\n",
      "          [-0., 0., -0.,  ..., 0., 0., -0.]],\n",
      "\n",
      "         [[-0., 0., -0.,  ..., 0., 0., 0.],\n",
      "          [-0., 0., 0.,  ..., 0., -0., 0.],\n",
      "          [-0., 0., 0.,  ..., 0., 0., -0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., -0., 0., 0.],\n",
      "          [0., -0., 0.,  ..., 0., -0., 0.],\n",
      "          [-0., 0., 0.,  ..., 0., 0., 0.]]]])\n"
     ]
    }
   ],
   "source": [
    "def apply_rotary_emb(xq,xk,pos_cis):\n",
    "    xq_=torch.view_as_complex(xq.float().reshape(*xq.shape[:-1], -1, 2))\n",
    "    xk_=torch.view_as_complex(xk.float().reshape(*xk.shape[:-1], -1, 2))\n",
    "    def unite_shape(pos_cis,x):\n",
    "        #对齐pos_cis和x的形状\n",
    "        #输入的x的形状是[bs,seqlen,head_num,head_dim]\n",
    "        #pos_cis的形状是[seqlen,head_dim]\n",
    "        #输出的形状是[bs,seqlen,head_num,head_dim]\n",
    "        ndim=x.ndim\n",
    "        assert 0<=1<ndim,\"x必须是至少2维的张量\"\n",
    "        # pos_cis必须和x的[bs,seqlen,head_num,head_dim//2],在seqlen和head_dim的形状上对齐，因为这是实际参加运算的部分\n",
    "        assert pos_cis.shape == (x.shape[1], x.shape[-1]), \"\"\n",
    "        shape = [d if i == 1 or i == ndim - 1 else 1 for i,  d in enumerate(x.shape)]\n",
    "        return pos_cis.view(*shape)\n",
    "    pos_cis=unite_shape(pos_cis,xq_)\n",
    "    xq_out= torch.view_as_real(xq_ * pos_cis).flatten(3)\n",
    "    xk_out= torch.view_as_real(xk_ * pos_cis).flatten(3)\n",
    "    return xq_out,xk_out\n",
    "\n",
    "# 测试下\n",
    "bs, seqlen, head_num, head_dim = 2, 100, 8, 64\n",
    "xq = torch.randn(bs, seqlen, head_num, head_dim)\n",
    "xk = torch.randn(bs, seqlen, head_num, head_dim)\n",
    "pos_cis = precompute_pos_cis(seqlen, head_dim)\n",
    "xq_out, xk_out = apply_rotary_emb(xq, xk, pos_cis)\n",
    "print(xq_out.shape)  # 应该是 [2, 100, 8, 64]\n",
    "print(xk_out.shape)  # 应该是 [2, 100, 8, 64]\n",
    "print(xq_out)\n",
    "print(xk_out)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddfd8d61",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "minimind",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
