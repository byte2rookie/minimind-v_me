{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c93f05c4",
   "metadata": {},
   "source": [
    "# 测试tokenizer的训练用脚本\n",
    "先data中下载相关数据集[tokenizer训练数据集](https://www.modelscope.cn/datasets/gongjy/minimind_dataset/resolve/master/pretrain_hq.jsonl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7e165b75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第一条数据内容：\n",
      "{\n",
      "  \"text\": \"<|im_start|>鉴别一组中文文章的风格和特点，例如官方、口语、文言等。需要提供样例文章才能准确鉴别不同的风格和特点。<|im_end|> <|im_start|>好的，现在帮我查一下今天的天气怎么样?今天的天气依据地区而异。请问你需要我帮你查询哪个地区的天气呢？<|im_end|> <|im_start|>打开闹钟功能，定一个明天早上七点的闹钟。好的，我已经帮您打开闹钟功能，闹钟将在明天早上七点准时响起。<|im_end|> <|im_start|>为以下场景写一句话描述：一个孤独的老人坐在公园长椅上看着远处。一位孤独的老人坐在公园长椅上凝视远方。<|im_end|> <|im_start|>非常感谢你的回答。请告诉我，这些数据是关于什么主题的？这些数据是关于不同年龄段的男女人口比例分布的。<|im_end|> <|im_start|>帮我想一个有趣的标题。这个挺有趣的：\\\"如何成为一名成功的魔术师\\\" 调皮的标题往往会吸引读者的注意力。<|im_end|> <|im_start|>回答一个问题，地球的半径是多少？地球的平均半径约为6371公里，这是地球自赤道到两极的距离的平均值。<|im_end|> <|im_start|>识别文本中的语气，并将其分类为喜悦、悲伤、惊异等。\\n文本：“今天是我的生日！”这个文本的语气是喜悦。<|im_end|>\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "#查看一下json数据结构\n",
    "import json\n",
    "data_path=\"../data/pretrain_hq.jsonl\"\n",
    "# 读取第一行并解析\n",
    "with open(data_path, 'r', encoding='utf-8') as f:\n",
    "    first_line = f.readline().strip()\n",
    "\n",
    "data = json.loads(first_line)\n",
    "print(\"第一条数据内容：\")\n",
    "print(json.dumps(data, ensure_ascii=False, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25099476",
   "metadata": {},
   "source": [
    "可以看到json表对应 text项是一个用户对话历史数据集\n",
    "以<|im_start|>开始，<|im_end|>结束，代表着用户/model的content项\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f4c3747",
   "metadata": {},
   "source": [
    "## 0.tokenizer基本流程\n",
    "normalize->pre_tokenization->encoder->decoder<br>\n",
    "Minimind采用了最基本的BPE方法，产生的是6400的size的token表，用来将文本encoder<br>\n",
    "[文章1-LLM实践-tokenizer](https://zhuanlan.zhihu.com/p/739078635)<br>\n",
    "[文章2-BPE,Sentencepiece等Tokenizer原理讲解](https://zhuanlan.zhihu.com/p/657047389)第6节开始的内容<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3327ae39",
   "metadata": {},
   "source": [
    "### 第一步：Normalization\n",
    "这一步是：\n",
    "- removing needless whitespace\n",
    "- lowercasing\n",
    "- removing accents<br>\n",
    "也就是移除空格，大小写重置，removeing_accents就是café Gómez résumé变成cafe Gomez resume\n",
    "\n",
    "### 第二步：Pre-tokenization\n",
    "tokenizer 是不可以直接在原始的文本上训练的，需要做一些处理，比如这里的将句子切分成一个个词汇。这个环节叫做 Pre-tokenization。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32da8b27",
   "metadata": {},
   "source": [
    "## 初始化tokenizer和tokenizer的训练器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2e44156f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import (\n",
    "    Tokenizer,\n",
    "    models,\n",
    "    trainers,\n",
    "    pre_tokenizers,\n",
    "    processors,\n",
    "    decoders,\n",
    ")\n",
    "tokenizer = Tokenizer(models.BPE())\n",
    "tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel(add_prefix_space=False)\n",
    "\n",
    "#定义特殊token\n",
    "special_tokens = [\"<|endoftext|>\",\n",
    "    \"<|im_start|>\",\n",
    "    \"<|im_end|>\",\n",
    "    \"<pad>\",\n",
    "    \"<mask>\"]\n",
    "\n",
    "# 定义训练器\n",
    "trainer= trainers.BpeTrainer(\n",
    "    vocab_size=6400,\n",
    "    special_tokens=special_tokens,\n",
    "    initial_alphabet=pre_tokenizers.ByteLevel.alphabet(),\n",
    "    show_progress=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6188a97",
   "metadata": {},
   "source": [
    "## 设计读取函数用于tokenizer训练读入数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "004065d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>鉴别一组中文文章的风格和特点，例如官方、口语、文言等。需要提供样例文章才能准确鉴别不同的风格和特点。<|im_end|> <|im_start|>好的，现在帮我查一下今天的天气怎么样?今天的天气依据地区而异。请问你需要我帮你查询哪个地区的天气呢？<|im_end|> <|im_start|>打开闹钟功能，定一个明天早上七点的闹钟。好的，我已经帮您打开闹钟功能，闹钟将在明天早上七点准时响起。<|im_end|> <|im_start|>为以下场景写一句话描述：一个孤独的老人坐在公园长椅上看着远处。一位孤独的老人坐在公园长椅上凝视远方。<|im_end|> <|im_start|>非常感谢你的回答。请告诉我，这些数据是关于什么主题的？这些数据是关于不同年龄段的男女人口比例分布的。<|im_end|> <|im_start|>帮我想一个有趣的标题。这个挺有趣的：\"如何成为一名成功的魔术师\" 调皮的标题往往会吸引读者的注意力。<|im_end|> <|im_start|>回答一个问题，地球的半径是多少？地球的平均半径约为6371公里，这是地球自赤道到两极的距离的平均值。<|im_end|> <|im_start|>识别文本中的语气，并将其分类为喜悦、悲伤、惊异等。\n",
      "文本：“今天是我的生日！”这个文本的语气是喜悦。<|im_end|>\n",
      "<|im_start|>根据输入的内容，编写一个类别标签。\n",
      "这是一篇介绍如何阅读心电图的文章类别标签: 医学/心电图阅读指南<|im_end|> <|im_start|>帮我搜索一下最近的天气情况。当然，我可以帮您搜索最新的天气情况。请问您需要查询哪个城市的天气情况呢？<|im_end|> <|im_start|>帮我讲一个令人开心的笑话。好的，我帮您讲一个关于细菌的笑话。为什么细菌不会上网？因为连接总是断开了！<|im_end|> <|im_start|>现在给我生成一首关于大海的五言诗。碧波万顷月满天，海天相接处天地间。波涛滚滚江山美，海鸟翱翔日月闲。<|im_end|> <|im_start|>谢谢你，这篇文章很有用。不客气，我很高兴能够为您提供帮助。如果您还有其他问题或需求，随时可以对我说。<|im_end|> <|im_start|>你好，我想下载一个视频编辑软件，你有什么推荐吗？您好！当然，有很多选择。您想要免费软件还是愿意付费？<|im_end|> <|im_start|>为什么我的程序不输出正确结果？可能是代码逻辑有误，或者输入数据有误，需要仔细调试代码逻辑和输入数据。<|im_end|> <|im_start|>谢谢你的回答。现在我想知道这场比赛的具体时间和地点。这场比赛的时间是北京时间10月4日，地点是上海。<|im_end|>\n"
     ]
    }
   ],
   "source": [
    "#采用的训练方法是train_from_iterator\n",
    "#定义一个迭代器函数读取数据集\n",
    "def read_data(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            data = json.loads(line.strip())\n",
    "            yield data['text']  # 假设每行数据有一个'text'字段\n",
    "#测试一下读取的方法\n",
    "data_path=\"../data/pretrain_hq.jsonl\"\n",
    "#打印两条数据\n",
    "data_iter = read_data(data_path)\n",
    "for _ in range(2):\n",
    "    print(next(data_iter))\n",
    "del data_iter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b8d00c4",
   "metadata": {},
   "source": [
    "## 开始训练\n",
    "大概训练耗时33min左右"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "30327af3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['../model/vocab.json', '../model/merges.txt']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_path=\"../data/pretrain_hq.jsonl\"\n",
    "texts= read_data(data_path)\n",
    "#开始训练\n",
    "tokenizer.train_from_iterator(texts, trainer=trainer)\n",
    "#设置解码器\n",
    "tokenizer.decoder= decoders.ByteLevel()\n",
    "#验证一下decoder效果\n",
    "assert tokenizer.token_to_id(\"<|endoftext|>\") == 0\n",
    "assert tokenizer.token_to_id(\"<|im_start|>\") == 1\n",
    "assert tokenizer.token_to_id(\"<|im_end|>\") == 2\n",
    "assert tokenizer.token_to_id(\"<pad>\") == 3\n",
    "assert tokenizer.token_to_id(\"<mask>\") == 4\n",
    "#保存tokenizer\n",
    "import os\n",
    "save_path = \"../model\"\n",
    "os.makedirs(save_path, exist_ok=True)\n",
    "tokenizer.save(os.path.join(save_path, \"tokenizer.json\"))\n",
    "tokenizer.model.save(save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca5ea13b",
   "metadata": {},
   "source": [
    "# 手动创建一份配置文件\n",
    "[关于配置文件](https://blog.csdn.net/xiezhipu/article/details/145585777)<br>\n",
    "[关于normalizer的配置说明](https://blog.csdn.net/weixin_49346755/article/details/126496833)<br>\n",
    "[关于post_process的配置说明](https://blog.csdn.net/weixin_49346755/article/details/126499720)<br>\n",
    "[chat_template的设计规则](https://www.guyuehome.com/detail?id=1888166611628642305)<br>\n",
    "配置文件的作用主要是记录tokenizer的normalize,pre_process,post_process,template,special_token等tokenizer的关键参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a717a9f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer training completed and saved.\n"
     ]
    }
   ],
   "source": [
    "# 手动创建配置文件\n",
    "config = {\n",
    "        \"add_bos_token\": False,\n",
    "        \"add_eos_token\": False,\n",
    "        \"add_prefix_space\": False,\n",
    "        \"added_tokens_decoder\": {\n",
    "            \"0\": {\n",
    "                \"content\": \"<|endoftext|>\",\n",
    "                \"lstrip\": False,\n",
    "                \"normalized\": False,\n",
    "                \"rstrip\": False,\n",
    "                \"single_word\": False,\n",
    "                \"special\": True\n",
    "            },\n",
    "            \"1\": {\n",
    "                \"content\": \"<|im_start|>\",\n",
    "                \"lstrip\": False,\n",
    "                \"normalized\": False,\n",
    "                \"rstrip\": False,\n",
    "                \"single_word\": False,\n",
    "                \"special\": True\n",
    "            },\n",
    "            \"2\": {\n",
    "                \"content\": \"<|im_end|>\",\n",
    "                \"lstrip\": False,\n",
    "                \"normalized\": False,\n",
    "                \"rstrip\": False,\n",
    "                \"single_word\": False,\n",
    "                \"special\": True\n",
    "            },\n",
    "            \"3\": {\n",
    "                \"content\": \"<pad>\",\n",
    "                \"lstrip\": False,\n",
    "                \"normalized\": False,\n",
    "                \"rstrip\": False,\n",
    "                \"single_word\": False,\n",
    "                \"special\": True\n",
    "            },\n",
    "            \"4\": {\n",
    "                \"content\": \"<mask>\",\n",
    "                \"lstrip\": False,\n",
    "                \"normalized\": False,\n",
    "                \"rstrip\": False,\n",
    "                \"single_word\": False,\n",
    "                \"special\": True\n",
    "            }\n",
    "        },\n",
    "        \"additional_special_tokens\": [ \"<pad>\", \"<mask>\",\"<|多余的flag|>\",\"<|zzy|>\"],\n",
    "        \"bos_token\": \"<|im_start|>\",\n",
    "        \"clean_up_tokenization_spaces\": False,\n",
    "        \"eos_token\": \"<|im_end|>\",\n",
    "        \"legacy\": True,\n",
    "        \"model_max_length\": 32768,\n",
    "        \"pad_token\": \"<|endoftext|>\",\n",
    "        \"sp_model_kwargs\": {},\n",
    "        \"spaces_between_special_tokens\": False,\n",
    "        \"tokenizer_class\": \"PreTrainedTokenizerFast\",\n",
    "        \"unk_token\": \"<|endoftext|>\",\n",
    "        \"chat_template\": \"{% if messages[0]['role'] == 'system' %}{% set system_message = messages[0]['content'] %}{{ '<|im_start|>system\\\\n' + system_message + '<|im_end|>\\\\n' }}{% else %}{{ '<|im_start|>system\\\\nYou are a helpful assistant<|im_end|>\\\\n' }}{% endif %}{% for message in messages %}{% set content = message['content'] %}{% if message['role'] == 'user' %}{{ '<|im_start|>user\\\\n' + content + '<|im_end|>\\\\n<|im_start|>assistant\\\\n' }}{% elif message['role'] == 'assistant' %}{{ content + '<|im_end|>' + '\\\\n' }}{% endif %}{% endfor %}\"\n",
    "    }\n",
    "\n",
    "save_path = \"../model\"\n",
    "os.makedirs(save_path, exist_ok=True)\n",
    "# 保存配置文件\n",
    "with open(os.path.join(save_path, \"tokenizer_config.json\"), \"w\", encoding=\"utf-8\") as config_file:\n",
    "    json.dump(config, config_file, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(\"Tokenizer training completed and saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b559a0fc",
   "metadata": {},
   "source": [
    "### 典型问题\n",
    "config的added_tokens_decoder里<br>\n",
    "我的\"<|多余的flag|>\",\"<|zzy|>\"其实没有对应的encoder，所以会产生不对应的问题，这样的做法是错误的<br>\n",
    "具体添加特殊标记的方法请参考模型的官方链接\n",
    "[参考1：如何扩充词表](https://zhuanlan.zhihu.com/p/704346193#:~:text=%E7%AE%80%E5%8D%95%E6%9D%A5%E8%AF%B4%EF%BC%8C%E8%AF%BB%E5%85%A5%20tokenizer%20model%E4%B9%8B%E5%90%8E%EF%BC%8C%E8%B0%83%E7%94%A8%20tokenizer%20%E7%9A%84%20add_special_tokens%20%E6%96%B9%E6%B3%95%E7%BB%99%20tokenizer,model%20%E7%9A%84%20embedding%20size%EF%BC%8C%E9%80%9A%E8%BF%87%E8%B0%83%E7%94%A8%20model%20%E7%9A%84%20resize_token_embeddings%20%E6%96%B9%E6%B3%95%E6%9D%A5%E5%AE%9E%E7%8E%B0%E8%BF%99%E4%B8%80%E7%82%B9%E3%80%82)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0122674c",
   "metadata": {},
   "source": [
    "#### template说明\n",
    "chat_template结构：\n",
    "```\n",
    "{% if messages[0]['role'] == 'system' %}\n",
    "  {% set system_message = messages[0]['content'] %}\n",
    "  {{ '<|im_start|>system\\n' + system_message + '<|im_end|>\\n' }}\n",
    "{% else %}\n",
    "  {{ '<|im_start|>system\\nYou are a helpful assistant<|im_end|>\\n' }}\n",
    "{% endif %}\n",
    "\n",
    "{% for message in messages %}\n",
    "  {% set content = message['content'] %}\n",
    "  {% if message['role'] == 'user' %}\n",
    "    {{ '<|im_start|>user\\n' + content + '<|im_end|>\\n<|im_start|>assistant\\n' }}\n",
    "  {% elif message['role'] == 'assistant' %}\n",
    "    {{ content + '<|im_end|>' + '\\n' }}\n",
    "  {% endif %}\n",
    "{% endfor %}\n",
    "```\n",
    "\n",
    "后续在读入我们的SFT的多轮对话数据集时候，就会用chat_template来转换数据集格式的。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbaf98b4",
   "metadata": {},
   "source": [
    "#### 评估tokenizer的效果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c022cf57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>system\n",
      "你是一个优秀的聊天机器人，总是给我正确的回应！<|im_end|>\n",
      "<|im_start|>user\n",
      "你来自哪里？<|im_end|>\n",
      "<|im_start|>assistant\n",
      "我来自地球<|im_end|>\n",
      "\n",
      "Tokenizer vocabulary size: 6400\n",
      "Tokenizer actual size: 6402\n",
      "length of model inputs: 46\n",
      "Input IDs: [1, 87, 93, 307, 73, 81, 203, 397, 924, 5235, 3317, 2117, 265, 2603, 1132, 2599, 703, 472, 997, 2, 203, 1, 89, 87, 3709, 203, 397, 2722, 3016, 425, 2, 203, 1, 69, 87, 87, 77, 307, 3924, 88, 203, 301, 2722, 1284, 2, 203]\n",
      "Response matches new prompt: True\n",
      "Tokenizer special tokens:\n",
      "{'bos_token': '<|im_start|>', 'eos_token': '<|im_end|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>', 'additional_special_tokens': ['<|多余的flag|>', '<|zzy|>']}\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "# 评估tokenizer的效果\n",
    "tokenizer_path=\"../model/\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_path, use_fast=True)\n",
    "\n",
    "#后续SFT数据的格式\n",
    "messages = [\n",
    "        {\"role\": \"system\", \"content\": \"你是一个优秀的聊天机器人，总是给我正确的回应！\"},\n",
    "        {\"role\": \"user\", \"content\": '你来自哪里？'},\n",
    "        {\"role\": \"assistant\", \"content\": '我来自地球'}\n",
    "    ]\n",
    "new_prompt = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False\n",
    "    ) #会把原始的dict转化为template格式的string\n",
    "print(new_prompt)\n",
    "\n",
    "#tokenizer理论词表长度\n",
    "print(\"Tokenizer vocabulary size:\", tokenizer.vocab_size)\n",
    "#tokenizer实际长度\n",
    "len_tokenizer = len(tokenizer)\n",
    "print(\"Tokenizer actual size:\", len_tokenizer)\n",
    "\n",
    "#encoder测试\n",
    "model_inputs=tokenizer(new_prompt)\n",
    "print(\"length of model inputs:\", len(model_inputs['input_ids']))\n",
    "input_ids= model_inputs['input_ids']\n",
    "print(\"Input IDs:\", input_ids)\n",
    "response= tokenizer.decode(input_ids, skip_special_tokens=False)\n",
    "#比对response和new_prompt是否一致\n",
    "print(\"Response matches new prompt:\", response == new_prompt)\n",
    "\n",
    "#打印tokenizer的特殊token\n",
    "print(\"Tokenizer special tokens:\")\n",
    "print(tokenizer.special_tokens_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3a6bcf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#至此，tokenizer的训练和验证已经完成。可以看到，tokenizer能够正确地处理输入文本，并且生成的token与预期一致。接下来可以将这个tokenizer应用于模型训练或推理任务中。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "minimind",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
