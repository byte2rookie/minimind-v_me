{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3348d092",
   "metadata": {},
   "source": [
    "# 搞清基本构成\n",
    "- tokenizer\n",
    "- Embedding\n",
    "- Position Embedding\n",
    "- Block\n",
    "    - RMSNorm\n",
    "    - FFN\n",
    "- decoder\n",
    "\n",
    "![img](../images/LLM-structure.png)\n",
    "\n",
    "接下来自底向上进行复现即可"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcf50134",
   "metadata": {},
   "source": [
    "## Embedding\n",
    "主要利用的是nn.Embedding即可"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e7af64ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "#假设tokenizer处理后，得到的是(bs,seqlen,vocab_size)的tensor\n",
    "import torch\n",
    "from torch import nn\n",
    "class Embedding(nn.Module):\n",
    "    def __init__(self,vocab_size,embed_dim):\n",
    "        super(Embedding,self).__init__()\n",
    "        self.embedding=nn.Embedding(vocab_size,embed_dim)\n",
    "    def forward(self,x):\n",
    "        return self.embedding(x)\n",
    "\n",
    "#进行测试\n",
    "#输入的tensor为（2，3，64）\n",
    "#输出的tensor为（2，3，8）\n",
    "# vocab_size=64\n",
    "# seqlen=3\n",
    "# embed_dim=8\n",
    "# bs=2\n",
    "# input_ids=torch.randint(0,vocab_size,(bs,seqlen))\n",
    "# embedding=Embedding(vocab_size=vocab_size,embed_dim=embed_dim)\n",
    "# output=embedding(input_ids)\n",
    "# print(\"input_ids:\", input_ids)\n",
    "# print(\"output:\", output)\n",
    "# print(\"input_ids shape:\", input_ids.shape)\n",
    "# print(\"output shape:\", output.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56e5937f",
   "metadata": {},
   "source": [
    "## RotaryEmbedding\n",
    "旋转编码，需要参考公式和模型定义\n",
    "### Rotary Position Embedding, RoPE\n",
    "\n",
    "旋转位置编码是一种能将相对位置信息集成到 self-attention 中, 进而提升 transformer 架构性能的位置编码方式, 和绝对位置编码相比, RoPE 具有很好的外推性, 是目前的主流位置编码方式.\n",
    "\n",
    "外推性的解释, 通俗来说就是训练的时候限制了 512 的上下文长度，那么推理时如果面对超过该长度的文本，LLM 可能无法正确处理.\n",
    "\n",
    "- **绝对位置编码**\n",
    "\n",
    "绝对位置编码是早期 Transformer 架构采用的绝对位置编码方案，及那个每个位置映射为固定的向量表示.\n",
    "\n",
    "$$f_{t:t\\in\\{q,k,v\\}}(\\boldsymbol{x}_i,i)=\\boldsymbol{W}_{t:t\\in\\{q,k,v\\}}(\\boldsymbol{x}_i+\\boldsymbol{p}_i)$$\n",
    "\n",
    "其中编码向量 $p_i$ 的计算使用如下公式：\n",
    "\n",
    "$$\\boldsymbol{p}_{i,2t}=\\sin\\left(k/1000^{2t/d}\\right), \\boldsymbol{p}_{i,2t+1}=\\cos\\left(k/1000^{2t/d}\\right)$$\n",
    "\n",
    "正如其名，绝对位置编码只考虑了输入序列中的绝对位置关系，对于 token 之间的相对信息则没有纳入考虑.\n",
    "\n",
    "- **旋转位置编码**\n",
    "\n",
    "假定 query 和 key 的内积操作可以被函数 g 表示，该函数 g 的输入是词嵌入向量 $x_m, x_n$ 和它们之间的相对位置 $m-n$:\n",
    "\n",
    "$$<f_q(x_m ,m), f_k(x_n, n)>=g(x_m, x_n, m, n)$$\n",
    "\n",
    "旋转位置编码就是找到一个使上式成立的位置编码方式. \n",
    "\n",
    "出于认识的目的，我们省略复杂的数学推导，直接看 RoPE 的的结论：\n",
    "\n",
    "存在这样一个正交矩阵：\n",
    "\n",
    "$$\\boldsymbol{R}_{\\Theta,m}^d=\\underbrace{\\begin{pmatrix}\\cos m\\theta_0&-\\sin m\\theta_0&0&0&\\cdots&0&0\\\\\\sin m\\theta_0&\\cos m\\theta_0&0&0&\\cdots&0&0\\\\0&0&\\cos m\\theta_1&-\\sin m\\theta_1&\\cdots&0&0\\\\0&0&\\sin m\\theta_1&\\cos m\\theta_1&\\cdots&0&0\\\\\\vdots&\\vdots&\\vdots&\\vdots&\\ddots&\\vdots&\\vdots\\\\0&0&0&0&\\cdots&\\cos m\\theta_{d/2-1}&-\\sin m\\theta_{d/2-1}&-\\sin m\\theta_{d/2-1}\\end{pmatrix}}_{\\boldsymbol{W}_m}$$\n",
    "\n",
    "其中，$\\Theta=\\left\\{\\theta_i=10000^{-2(i-1)/d},i\\in[1,2,\\ldots,d/2]\\right\\}$\n",
    "\n",
    "我们可以将 query 和 key 的内积操作转换为与原始向量 $x$ 相关的以下等价形式：\n",
    "\n",
    "$$\n",
    "\\boldsymbol{q}_m^\\mathbf{T}\\boldsymbol{k}_n=\\left(\\boldsymbol{R}_{\\Theta,m}^d\\boldsymbol{W}_q\\boldsymbol{x}_m\\right)^\\mathbf{T}\\left(\\boldsymbol{R}_{\\Theta,n}^d\\boldsymbol{W}_k\\boldsymbol{x}_n\\right)=\\boldsymbol{x}_m^\\mathbf{T}\\boldsymbol{W}_q\\boldsymbol{R}_{\\Theta,n-m}^d\\boldsymbol{W}_k\\boldsymbol{x}_n\n",
    "$$\n",
    "\n",
    "其中， $\\boldsymbol{R}_{\\Theta,n-m}^d=\\left(\\boldsymbol{R}_{\\Theta,m}^d\\right)^\\mathbf{T}\\boldsymbol{R}_{\\Theta,n}^d$.\n",
    "\n",
    "由于 $\\boldsymbol{R}_{\\Theta,m}^d$ 的稀疏性，直接使用矩阵乘法会浪费算力，因此代码中采用下述方式实现：\n",
    "\n",
    "$$\\boldsymbol{R}_{\\Theta,m}^{d}\\boldsymbol{x}=\\begin{pmatrix}x_{0}\\\\x_{1}\\\\x_{2}\\\\x_{3}\\\\\\vdots\\\\x_{d-2}\\\\x_{d-1}\\end{pmatrix}\\otimes\\begin{pmatrix}\\cos m\\theta_{0}\\\\\\cos m\\theta_{0}\\\\\\cos m\\theta_{1}\\\\\\cos m\\theta_{1}\\\\\\vdots\\\\\\cos m\\theta_{d/2-1}\\\\\\cos m\\theta_{d/2-1}\\end{pmatrix}+\\begin{pmatrix}-x_{1}\\\\x_{0}\\\\-x_{3}\\\\x_{2}\\\\\\vdots\\\\-x_{d-1}\\\\x_{d-2}\\end{pmatrix}\\otimes\\begin{pmatrix}\\sin m\\theta_{0}\\\\\\sin m\\theta_{0}\\\\\\sin m\\theta_{1}\\\\\\sin m\\theta_{1}\\\\\\vdots\\\\\\sin m\\theta_{d/2-1}\\\\\\sin m\\theta_{d/2-1}\\end{pmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9632cab3",
   "metadata": {},
   "source": [
    "此处的ROPE的实现主要参考的是LLama的RoPE实现\n",
    "[LLAMA实现](https://blog.csdn.net/m0_55846238/article/details/145728695)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d041dadb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pos_cis: tensor([[ 1.0000+0.0000e+00j,  1.0000+0.0000e+00j,  1.0000+0.0000e+00j,\n",
      "          1.0000+0.0000e+00j],\n",
      "        [ 0.5403+8.4147e-01j,  0.9984+5.6204e-02j,  1.0000+3.1623e-03j,\n",
      "          1.0000+1.7783e-04j],\n",
      "        [-0.4161+9.0930e-01j,  0.9937+1.1223e-01j,  1.0000+6.3245e-03j,\n",
      "          1.0000+3.5566e-04j],\n",
      "        [-0.9900+1.4112e-01j,  0.9858+1.6790e-01j,  1.0000+9.4867e-03j,\n",
      "          1.0000+5.3348e-04j],\n",
      "        [-0.6536-7.5680e-01j,  0.9748+2.2304e-01j,  0.9999+1.2649e-02j,\n",
      "          1.0000+7.1131e-04j],\n",
      "        [ 0.2837-9.5892e-01j,  0.9607+2.7748e-01j,  0.9999+1.5811e-02j,\n",
      "          1.0000+8.8914e-04j],\n",
      "        [ 0.9602-2.7942e-01j,  0.9436+3.3104e-01j,  0.9998+1.8973e-02j,\n",
      "          1.0000+1.0670e-03j]])\n",
      "pos_cis shape: torch.Size([7, 4])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "def precompute_pos_cis(dim:int,seqlen=2048,theta=1e5):\n",
    "    #这个函数是用来求出所有的mtheta的\n",
    "    #首先就需要制备theta的值，参考上面的公式\n",
    "    #theta序列\n",
    "    freqs=1.0/(theta**(torch.arange(0,dim,2)[:dim//2].float()/dim))\n",
    "    #m序列\n",
    "    m=torch.arange(seqlen,device=freqs.device)\n",
    "    #求出m x theta的大表\n",
    "    freqs=torch.outer(m,freqs).float()\n",
    "    #用极坐标表示cos + isin形式的mtheta\n",
    "    pos_cis=torch.polar(torch.ones_like(freqs),freqs)\n",
    "    #pos_cis的形状是(seqlen,dim)\n",
    "    return pos_cis\n",
    "\n",
    "#测试查看一下制出的表格\n",
    "pos_cis=precompute_pos_cis(dim=8,seqlen=7,theta=1e5)\n",
    "print(\"pos_cis:\", pos_cis)\n",
    "print(\"pos_cis shape:\", pos_cis.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5087a000",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xq_,shape torch.Size([2, 3, 2, 4])\n",
      "xk_,shape torch.Size([2, 3, 2, 4])\n",
      "xq_.shape= torch.Size([2, 3, 2, 4])\n",
      "pos_cis shape: torch.Size([1, 3, 1, 4])\n",
      "xq_ shape: torch.Size([2, 3, 2, 4])\n",
      "xk_ shape: torch.Size([2, 3, 2, 4])\n",
      "xq shape: torch.Size([2, 3, 2, 8])\n",
      "xk shape: torch.Size([2, 3, 2, 8])\n",
      "xq_out_shape: torch.Size([2, 3, 2, 8])\n",
      "xk_out_shape: torch.Size([2, 3, 2, 8])\n",
      "pos_cis: tensor([[ 1.0000+0.0000e+00j,  1.0000+0.0000e+00j,  1.0000+0.0000e+00j,\n",
      "          1.0000+0.0000e+00j],\n",
      "        [ 0.5403+8.4147e-01j,  0.9984+5.6204e-02j,  1.0000+3.1623e-03j,\n",
      "          1.0000+1.7783e-04j],\n",
      "        [-0.4161+9.0930e-01j,  0.9937+1.1223e-01j,  1.0000+6.3245e-03j,\n",
      "          1.0000+3.5566e-04j]])\n",
      "pos_cis shape: torch.Size([3, 4])\n"
     ]
    }
   ],
   "source": [
    "def apply_rotary_emb(xq,xk,pos_cis):\n",
    "    #将旋转编码应用到输入的tensor上\n",
    "    xq_=torch.view_as_complex(xq.float().reshape(*xq.shape[:-1],-1,2))\n",
    "    print(\"xq_,shape\",xq_.shape)\n",
    "    xk_=torch.view_as_complex(xk.float().reshape(*xk.shape[:-1],-1,2))\n",
    "    print(\"xk_,shape\",xk_.shape)\n",
    "    #将pos_cis的维度调整为(xq_.shape[0],xq_.shape[1],-1,2)\n",
    "    def unite_shape(pos_cis,  x):\n",
    "        ndim = x.ndim\n",
    "        assert 0 <= 1 < ndim\n",
    "        print(\"xq_.shape=\",xq_.shape)\n",
    "        assert pos_cis.shape == (x.shape[1],  x.shape[-1]),f\"pos_cis.shape:({pos_cis.shape}),(x.shape[1],  x.shape[-1])={(x.shape[1],  x.shape[-1])}\"\n",
    "        shape = [d if i == 1 or i == ndim - 1 else 1 for i,  d in enumerate(x.shape)]\n",
    "        return pos_cis.view(*shape)\n",
    "    pos_cis = unite_shape(pos_cis, xq_)\n",
    "    #将pos_cis应用到xq_和xk_上(和输入对齐)\n",
    "    print(\"pos_cis shape:\", pos_cis.shape)\n",
    "    print(\"xq_ shape:\", xq_.shape)\n",
    "    print(\"xk_ shape:\", xk_.shape)\n",
    "\n",
    "    xq_out=torch.view_as_real(xq_ * pos_cis).flatten(3)\n",
    "    xk_out=torch.view_as_real(xk_ * pos_cis).flatten(3)\n",
    "    return xq_out, xk_out\n",
    "#测试一下apply_rotary_emb函数\n",
    "xq = torch.randn(2, 3, 2,8)  # (bs, seqlen, dim)\n",
    "xk = torch.randn(2, 3, 2,8)  # (bs, seqlen, dim)\n",
    "pos_cis = precompute_pos_cis(dim=8, seqlen=3, theta=1e5)\n",
    "xq_out, xk_out = apply_rotary_emb(xq, xk, pos_cis)\n",
    "print(\"xq shape:\", xq.shape)  # 应该是 (bs, seqlen, dim)\n",
    "print(\"xk shape:\", xk.shape)  # 应该是 (bs, seqlen, dim)\n",
    "\n",
    "print(\"xq_out_shape:\", xq_out.shape)  # 应该是 (bs, seqlen, dim\n",
    "print(\"xk_out_shape:\", xk_out.shape)  # 应该是 (bs, seqlen, dim\n",
    "# print(\"xq_out:\", xq_out)\n",
    "# print(\"xk_out:\", xk_out)\n",
    "print(\"pos_cis:\", pos_cis)\n",
    "print(\"pos_cis shape:\", pos_cis.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcda7106",
   "metadata": {},
   "source": [
    "## Attention Block\n",
    "MiniMind主要采用的GQA，所以我们采用GQA来进行复现即可，其中要考虑到KV_cache机制和是否使用flash_attention的scale_dot_attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ceeafef2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x shape: torch.Size([2, 3, 4, 8])\n",
      "x_repeated shape: torch.Size([2, 3, 12, 8])\n"
     ]
    }
   ],
   "source": [
    "def repeat_kv_heads(x,rep_num):\n",
    "    #该函数主要用来对齐kv和q的维度的,因为kv一般为(bs,seqlen,kv_head_num,head_dim)而q一般为(bs,seqlen,head_num,head_dim)\n",
    "    #所以需要将kv的head_num重复rep_num次、\n",
    "    bs,seqlen,kv_head_num,head_dim=x.shape\n",
    "    if rep_num == 1:\n",
    "        return x\n",
    "    return (\n",
    "        x[:,:,:,None,:].expand(bs,seqlen,kv_head_num,rep_num,head_dim)\n",
    "        .reshape(bs,seqlen,kv_head_num*rep_num,head_dim)\n",
    "    )\n",
    "#测试一下repeat_kv_heads函数\n",
    "x = torch.randn(2, 3, 4, 8)  # (\n",
    "# bs, seqlen, kv_head_num, head_dim)\n",
    "rep_num = 3\n",
    "x_repeated = repeat_kv_heads(x, rep_num)\n",
    "print(\"x shape:\", x.shape)  \n",
    "print(\"x_repeated shape:\", x_repeated.shape)  # 应该是 (bs, seqlen, kv_head_num * rep_num, head_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bbc5c78",
   "metadata": {},
   "source": [
    "这里我一直的疑惑是为什么Query和Value会进行位置编码，而Value不用进行位置编码\n",
    "现在的解答是：因为Query和Value本身的作用是产生特定的注意力分数，这个注意力分数要体现位置关系，这样才可以正确反映合适的句子的关系<br>\n",
    "而得到这个注意力分数以后，其注意力本身就已经参入了位置信息，所以Value可以不用再进行编码处理了。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bcf9f8c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xq_,shape torch.Size([2, 3, 8, 8])\n",
      "xk_,shape torch.Size([2, 3, 8, 8])\n",
      "xq_.shape= torch.Size([2, 3, 8, 8])\n",
      "pos_cis shape: torch.Size([1, 3, 1, 8])\n",
      "xq_ shape: torch.Size([2, 3, 8, 8])\n",
      "xk_ shape: torch.Size([2, 3, 8, 8])\n",
      "output shape: torch.Size([2, 3, 128])\n",
      "output: tensor([[[-1.0662e-01, -1.4046e-01, -1.6654e-01,  1.2609e-01,  3.1025e-01,\n",
      "          -1.6245e-01,  1.6930e-01,  1.7007e-01,  3.0667e-01,  1.0035e-01,\n",
      "          -6.2252e-02, -5.5633e-03,  3.1303e-02, -4.2615e-02,  0.0000e+00,\n",
      "           2.3188e-01,  1.6678e-01,  2.0312e-01, -3.5302e-02, -0.0000e+00,\n",
      "           2.9188e-02, -0.0000e+00, -8.3818e-02,  1.7378e-01,  4.8121e-02,\n",
      "          -2.6933e-01,  2.0837e-01, -1.0619e-01, -4.8884e-02, -1.3271e-01,\n",
      "           3.9362e-01, -9.6815e-02,  1.3962e-01,  6.8465e-02, -6.6692e-03,\n",
      "          -5.6169e-02, -3.6663e-01,  1.9279e-01, -1.8567e-01, -4.4651e-01,\n",
      "           0.0000e+00,  0.0000e+00, -1.2596e-01,  0.0000e+00, -3.1567e-01,\n",
      "          -2.1245e-01, -4.8691e-02,  8.1511e-02, -2.5524e-01,  4.6297e-02,\n",
      "           3.4736e-02, -1.6003e-01,  4.0135e-01, -3.1045e-01,  1.7786e-01,\n",
      "          -4.0941e-02,  1.4691e-01, -1.3828e-01,  3.9900e-01, -3.0565e-01,\n",
      "           2.2633e-01, -1.9163e-01, -1.6473e-02,  5.0730e-02,  0.0000e+00,\n",
      "          -1.9410e-01, -2.2559e-01,  3.7851e-01,  3.2669e-01, -0.0000e+00,\n",
      "          -2.3465e-01,  2.8888e-01,  1.1713e-01,  2.3721e-01,  9.0425e-02,\n",
      "           0.0000e+00, -2.6492e-01,  6.5056e-02, -0.0000e+00,  3.4103e-03,\n",
      "           3.8948e-02,  9.1850e-02,  1.5072e-01,  6.6342e-02,  5.7021e-02,\n",
      "           6.2560e-02, -3.8965e-02, -3.7326e-02, -2.0977e-01, -0.0000e+00,\n",
      "           2.3521e-01, -5.2948e-02,  5.3887e-02, -3.1850e-01, -0.0000e+00,\n",
      "          -1.9460e-01, -1.0605e-01,  1.4992e-01, -1.1486e-01,  2.0309e-01,\n",
      "          -4.1037e-01, -1.4853e-01, -1.1053e-03, -3.2599e-02,  1.1272e-01,\n",
      "          -3.3095e-01, -5.5968e-02, -8.9527e-02, -1.5235e-02, -3.3340e-01,\n",
      "           3.2287e-01, -2.9555e-01,  4.5823e-02, -3.1343e-01, -6.9357e-02,\n",
      "          -2.2959e-01, -0.0000e+00, -2.7940e-01, -1.2498e-01,  7.1808e-02,\n",
      "           2.4894e-02, -3.8989e-02, -2.3233e-01,  3.1292e-01,  3.8736e-02,\n",
      "           2.3175e-01,  3.1130e-01, -2.4860e-01],\n",
      "         [-1.9846e-01, -1.2240e-01, -1.4964e-01,  0.0000e+00,  1.8485e-01,\n",
      "          -6.5821e-02,  1.5920e-01,  1.7559e-01,  2.8224e-01,  1.1239e-01,\n",
      "           5.6139e-02,  1.3524e-02,  2.4442e-01, -3.1994e-02,  3.8656e-01,\n",
      "           1.2385e-01,  1.2384e-01,  2.2274e-01,  1.1458e-01, -4.8380e-02,\n",
      "          -0.0000e+00, -9.5504e-02,  3.1641e-02,  1.5358e-01,  1.2742e-01,\n",
      "          -3.4993e-01,  0.0000e+00, -2.3978e-01,  8.0681e-02, -1.8139e-01,\n",
      "           4.3177e-01, -1.7284e-01, -7.1681e-02,  1.5401e-01, -3.1520e-02,\n",
      "          -8.9930e-02, -2.6768e-01,  1.9747e-01, -4.0111e-01, -4.5722e-01,\n",
      "           1.2517e-01,  2.6329e-01, -2.7365e-01,  3.6977e-01, -3.4372e-01,\n",
      "          -2.5720e-01, -4.2107e-02, -1.0052e-01, -3.4581e-01,  3.1401e-02,\n",
      "          -9.3992e-02, -4.3693e-02,  4.9311e-01, -2.7287e-01,  3.2222e-01,\n",
      "          -9.9982e-02,  2.8507e-01, -1.9077e-01,  4.5199e-01, -2.9679e-01,\n",
      "           1.2224e-01, -1.5761e-01,  3.5877e-02, -1.3132e-02,  6.9935e-02,\n",
      "          -8.2096e-02, -2.8700e-01,  3.4477e-01,  4.3898e-01, -1.7011e-01,\n",
      "          -0.0000e+00,  2.0061e-01,  3.1173e-01,  2.7120e-01,  1.5449e-01,\n",
      "           2.5158e-02, -3.2334e-01,  1.6954e-01, -4.4859e-01,  1.1558e-01,\n",
      "          -9.9786e-02,  1.9366e-01,  1.4904e-01,  4.5022e-02,  4.3569e-03,\n",
      "          -8.8692e-02,  1.4547e-02,  0.0000e+00, -2.5919e-01, -5.7856e-01,\n",
      "           2.3097e-01, -1.5471e-02,  5.3506e-02, -4.0022e-01, -1.7143e-01,\n",
      "          -0.0000e+00, -0.0000e+00,  3.4348e-02, -9.8328e-02,  2.0890e-01,\n",
      "          -5.6262e-01, -2.0794e-01, -7.9679e-02,  4.1313e-02,  2.2963e-01,\n",
      "          -3.1176e-01, -0.0000e+00, -4.6984e-02, -9.9724e-03, -3.2466e-01,\n",
      "           2.8917e-01, -0.0000e+00,  1.0217e-01, -3.6787e-01, -1.3792e-01,\n",
      "          -2.3198e-01, -0.0000e+00, -2.0318e-01, -1.0757e-01,  1.4159e-03,\n",
      "           0.0000e+00, -8.5387e-03, -1.1877e-01,  0.0000e+00, -7.4775e-02,\n",
      "           1.9332e-01,  2.1026e-01, -1.0869e-01],\n",
      "         [-0.0000e+00, -1.3490e-01, -1.7601e-01,  9.7762e-02,  1.8789e-01,\n",
      "          -5.9850e-02,  5.9361e-02,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          -1.0869e-01, -2.5608e-02,  1.6271e-01,  2.8158e-04,  2.8101e-01,\n",
      "          -5.8873e-02,  1.8928e-01,  1.1744e-01,  0.0000e+00, -6.4331e-03,\n",
      "          -8.2659e-02,  8.9506e-02, -1.8590e-03,  1.2761e-01,  3.8016e-02,\n",
      "          -1.8993e-01,  7.7852e-02, -2.0405e-01,  2.8753e-02, -1.2484e-01,\n",
      "           3.4421e-01,  1.9262e-02,  3.3206e-02, -0.0000e+00, -2.4339e-02,\n",
      "          -0.0000e+00, -3.3287e-01,  0.0000e+00, -2.2139e-01, -4.4399e-01,\n",
      "           1.8632e-02,  3.1287e-01, -1.8160e-01,  3.2128e-01, -1.7379e-01,\n",
      "          -1.8069e-01, -9.0826e-02,  7.1035e-02, -1.4747e-01,  3.3379e-02,\n",
      "           3.5351e-02, -1.2093e-01,  4.1884e-01, -9.3305e-02,  2.2421e-01,\n",
      "           7.9147e-02,  2.7431e-01, -8.0849e-02,  3.4211e-01, -2.8523e-01,\n",
      "           2.7620e-01, -1.2219e-01, -4.0401e-02,  1.1853e-02, -0.0000e+00,\n",
      "          -2.0516e-01, -2.7620e-01,  3.2056e-01,  3.6214e-01, -4.3005e-02,\n",
      "          -2.3260e-01,  2.2451e-01,  1.5955e-01,  1.7010e-01,  1.7508e-01,\n",
      "           3.6792e-02, -3.1531e-01,  1.3039e-01, -4.4616e-01, -5.5193e-03,\n",
      "          -3.1416e-02,  7.5375e-02,  1.4759e-01,  1.5270e-01,  1.4649e-01,\n",
      "           4.0364e-02,  1.0651e-01,  0.0000e+00, -2.0250e-01, -4.2253e-01,\n",
      "           1.5629e-01, -3.7767e-02,  1.4168e-01, -1.6542e-01, -5.4957e-02,\n",
      "          -2.2348e-01, -1.1306e-01,  2.0998e-01, -1.6848e-01,  2.1872e-01,\n",
      "          -3.4184e-01, -1.6575e-01, -3.8385e-02, -6.9010e-02,  0.0000e+00,\n",
      "          -2.0621e-01, -1.3123e-01, -7.3793e-02, -1.3101e-02, -0.0000e+00,\n",
      "           9.5857e-02, -1.8980e-01,  2.0872e-01, -2.9709e-01, -6.1700e-02,\n",
      "          -0.0000e+00, -6.2501e-02, -1.4946e-01, -4.8651e-02,  1.2942e-03,\n",
      "          -5.0605e-03, -2.6974e-02, -1.5724e-01,  1.9824e-01, -2.8673e-02,\n",
      "           0.0000e+00,  1.9741e-01, -2.6256e-01]],\n",
      "\n",
      "        [[-8.8124e-02, -1.3864e-01, -2.8028e-01,  2.6156e-01,  2.4467e-01,\n",
      "          -1.3776e-01,  1.1137e-01,  2.1151e-01,  2.7116e-01,  7.9714e-02,\n",
      "          -2.0327e-02,  2.4059e-02,  5.1070e-02, -4.5384e-02,  3.1256e-01,\n",
      "           6.5755e-04,  1.1032e-01,  2.8408e-01,  8.8438e-02,  2.7666e-02,\n",
      "           9.4673e-02,  2.2102e-02, -0.0000e+00,  2.7775e-01,  0.0000e+00,\n",
      "           6.5125e-02,  5.3017e-02, -8.9187e-02, -4.3211e-02, -9.1654e-02,\n",
      "           3.9521e-01, -1.0680e-01,  2.4649e-02, -8.3591e-02, -0.0000e+00,\n",
      "          -3.7883e-02, -3.0313e-01,  7.4010e-02, -1.7370e-01, -4.6619e-01,\n",
      "           7.9372e-02,  2.2330e-01, -1.0803e-01,  3.5045e-01, -2.5138e-01,\n",
      "          -2.6913e-01, -1.0732e-01,  6.9662e-02, -7.7120e-02,  4.7152e-02,\n",
      "          -4.0380e-02, -1.6882e-01,  4.0467e-01, -0.0000e+00,  1.9398e-01,\n",
      "          -7.8938e-02,  2.4005e-01, -1.1522e-01,  5.0041e-01, -0.0000e+00,\n",
      "           6.4334e-02, -1.6990e-01, -9.6559e-02, -4.0333e-02, -2.0861e-01,\n",
      "          -1.4024e-01, -2.5801e-01,  3.9114e-01,  0.0000e+00, -8.1957e-02,\n",
      "          -2.5176e-01,  2.8491e-01,  2.0200e-01,  2.9453e-01,  3.0196e-01,\n",
      "          -5.4234e-03, -4.3895e-01,  2.5742e-01, -3.4306e-01, -0.0000e+00,\n",
      "           2.3743e-02, -2.9606e-02,  2.2372e-01,  8.6557e-02,  1.3941e-01,\n",
      "          -3.3208e-02,  0.0000e+00,  7.9821e-02, -2.3382e-01, -5.4406e-01,\n",
      "           0.0000e+00, -6.5736e-02, -0.0000e+00, -4.9624e-02, -5.1115e-02,\n",
      "          -3.8963e-01, -2.0733e-01,  2.7179e-01, -2.4167e-01,  1.9335e-01,\n",
      "          -2.4152e-01, -1.7753e-01,  3.3464e-02, -9.9918e-02,  1.0213e-01,\n",
      "          -2.1107e-01, -0.0000e+00, -2.2082e-01, -1.0244e-01, -2.0643e-01,\n",
      "           3.7417e-01, -3.2687e-01,  1.4232e-01, -0.0000e+00,  4.0155e-02,\n",
      "          -4.5520e-01, -3.4717e-02, -3.1024e-01, -0.0000e+00, -4.3050e-02,\n",
      "           2.1321e-03, -4.3061e-02, -2.1112e-01,  2.9402e-01,  2.2403e-02,\n",
      "           1.3333e-01,  2.0475e-01, -2.7633e-01],\n",
      "         [-9.3148e-03, -1.8581e-01, -1.4812e-01,  1.8157e-01,  5.3049e-01,\n",
      "          -1.9475e-02,  0.0000e+00,  2.2963e-01,  3.3711e-01,  8.0376e-02,\n",
      "           2.2691e-01,  0.0000e+00,  3.0838e-01, -6.9526e-02,  2.5286e-01,\n",
      "          -1.0951e-01,  2.3207e-01,  1.1300e-01,  2.1620e-01,  5.4309e-02,\n",
      "           8.9633e-02, -1.4784e-01,  9.2583e-02,  1.4273e-01,  4.5572e-02,\n",
      "           0.0000e+00,  6.5046e-02, -3.5215e-01,  2.5756e-02, -0.0000e+00,\n",
      "           5.3154e-01, -2.2755e-01, -1.8569e-01, -4.8695e-02, -2.8360e-01,\n",
      "          -5.8152e-02, -3.0470e-01,  2.5375e-01, -1.5642e-01, -0.0000e+00,\n",
      "           5.2231e-02,  3.2765e-01, -7.6947e-03,  2.0508e-01, -0.0000e+00,\n",
      "          -0.0000e+00, -1.2385e-01,  2.6950e-01, -9.2269e-02,  1.5045e-01,\n",
      "          -9.0408e-02, -3.7513e-01,  3.9726e-01, -8.9046e-02,  1.5223e-01,\n",
      "           4.8132e-02,  3.8388e-01, -2.8879e-02,  0.0000e+00, -0.0000e+00,\n",
      "           5.6045e-02, -2.7475e-01, -1.7937e-01,  0.0000e+00, -2.8645e-01,\n",
      "          -0.0000e+00, -0.0000e+00,  4.3278e-01,  3.7541e-01, -3.1219e-02,\n",
      "          -2.8243e-01,  3.8614e-01,  2.7750e-01,  4.1334e-01,  4.0906e-01,\n",
      "          -2.4055e-02, -5.6638e-01,  2.5074e-01, -3.3859e-01, -6.6932e-02,\n",
      "          -4.6891e-02,  1.4372e-02,  2.7878e-01,  1.9778e-01,  7.9334e-02,\n",
      "           1.4858e-01,  2.8906e-02,  1.1650e-01, -0.0000e+00, -6.0774e-01,\n",
      "           1.3957e-01, -1.5328e-01,  1.8391e-01, -2.3694e-01,  3.8649e-02,\n",
      "          -5.2210e-01, -1.6023e-01,  2.1994e-01, -1.9895e-01,  1.9143e-01,\n",
      "          -4.3810e-01, -3.2609e-01, -1.2077e-01, -6.8095e-02,  1.1199e-01,\n",
      "          -4.4712e-02, -0.0000e+00, -2.4727e-01, -3.2450e-01, -1.2487e-01,\n",
      "           3.7517e-01, -2.7350e-01,  1.7473e-01, -2.9579e-01, -8.8178e-02,\n",
      "          -0.0000e+00, -0.0000e+00, -3.7556e-01, -3.5973e-01, -1.0358e-01,\n",
      "           4.8781e-03, -9.7285e-02, -9.3786e-02,  2.5205e-01, -2.6224e-02,\n",
      "           1.9408e-01,  3.7669e-01, -0.0000e+00],\n",
      "         [-1.7993e-01, -9.8160e-02, -9.7682e-02,  6.7023e-02,  1.1609e-01,\n",
      "          -2.5036e-01, -3.8782e-04,  1.7055e-01,  1.7377e-01,  1.6938e-01,\n",
      "           8.6624e-02, -5.0189e-02,  1.1132e-01, -1.3463e-02,  2.8234e-01,\n",
      "           7.1577e-03,  3.7917e-02,  1.8453e-01,  8.9191e-03,  9.4542e-02,\n",
      "          -1.1372e-01,  8.2323e-02, -3.6950e-02,  1.4060e-01,  1.2287e-01,\n",
      "          -3.6111e-03,  4.6103e-02, -0.0000e+00,  2.0179e-02, -1.5313e-01,\n",
      "           3.7512e-01, -7.5525e-02, -1.3883e-02, -7.0535e-02, -1.1176e-01,\n",
      "          -3.5042e-02, -1.8905e-01,  2.0170e-01, -1.4006e-01, -3.0727e-01,\n",
      "           3.7664e-02,  2.5359e-01, -2.7116e-01,  2.6870e-01, -1.3633e-01,\n",
      "          -2.6340e-01, -1.5963e-01, -0.0000e+00, -1.1477e-02,  1.0774e-01,\n",
      "          -4.8890e-02, -2.4249e-01,  3.5299e-01, -3.1961e-01,  6.8128e-02,\n",
      "          -1.3512e-01,  3.0149e-01, -1.6432e-01,  3.8279e-01, -3.4294e-01,\n",
      "           1.2996e-01, -1.7060e-01, -9.1383e-02,  2.4021e-02, -1.4807e-01,\n",
      "          -1.2040e-01, -2.3493e-01,  0.0000e+00,  3.6355e-01, -2.8296e-02,\n",
      "          -2.3165e-01,  1.3846e-01,  2.1922e-01,  3.0125e-01,  2.2023e-01,\n",
      "          -2.2350e-02, -3.5567e-01,  0.0000e+00, -2.2537e-01, -4.8723e-02,\n",
      "           9.7418e-02,  5.8205e-02,  1.8296e-01,  1.2378e-01,  2.3767e-01,\n",
      "          -2.2132e-02, -5.8306e-03,  0.0000e+00, -1.5703e-01, -4.3419e-01,\n",
      "           0.0000e+00, -1.3824e-01,  1.6824e-02, -2.5407e-01, -5.9195e-02,\n",
      "          -0.0000e+00, -1.4858e-01,  1.6449e-01, -4.2652e-02,  1.1234e-01,\n",
      "          -0.0000e+00, -2.0992e-01,  0.0000e+00, -1.0008e-02,  2.1732e-01,\n",
      "          -8.1077e-02, -1.0720e-01, -2.6166e-01, -9.5692e-02, -2.6087e-01,\n",
      "           2.6983e-01, -2.9814e-01,  3.5967e-02, -2.4134e-01,  0.0000e+00,\n",
      "          -3.3423e-01,  3.4122e-02, -1.8466e-01, -1.5790e-01, -2.7053e-02,\n",
      "          -1.1177e-01, -3.1806e-02, -1.9827e-01,  2.5214e-01, -5.7585e-02,\n",
      "           1.3582e-01,  1.5246e-01, -2.3221e-01]]], grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "class GroupQueryAttention(nn.Module):\n",
    "    def __init__(self,embed_dim,head_num,kv_head_num,dropout=0.1,Flash=False,max_seqlen=2048,training=True):\n",
    "        super(GroupQueryAttention,self).__init__()\n",
    "        self.embed_dim=embed_dim\n",
    "        self.head_num=head_num\n",
    "        self.kv_head_num=kv_head_num\n",
    "        #关于多头处理的设定\n",
    "        self.head_dim=embed_dim//head_num\n",
    "        assert self.head_dim * self.head_num == embed_dim, \"embed_dim must be divisible by head_num\"\n",
    "        self.rep_num=head_num//kv_head_num\n",
    "        assert self.rep_num * self.kv_head_num == self.head_num, \"head_num must be divisible by kv_head_num\"\n",
    "\n",
    "        #设置qkvo四个linear层\n",
    "        self.q_proj=nn.Linear(embed_dim,self.head_num*self.head_dim)\n",
    "        self.k_proj=nn.Linear(embed_dim,self.kv_head_num*self.head_dim)\n",
    "        self.v_proj=nn.Linear(embed_dim,self.kv_head_num*self.head_dim)\n",
    "        self.o_proj=nn.Linear(self.head_num*self.head_dim,self.embed_dim)\n",
    "\n",
    "        #dropout和flash_attention设置\n",
    "        self.training=training  #主要是在flash attention中使用,如果是tranning=True，则dropout会被使用，否则在推理模式下不会使用flash attention\n",
    "        self.dropout=dropout\n",
    "        self.attn_dropout=nn.Dropout(dropout)\n",
    "        self.res_dropout=nn.Dropout(dropout)\n",
    "        self.Flash=hasattr(F, 'scaled_dot_product_attention') and Flash\n",
    "        #注册因果掩码\n",
    "        #注意由于mask是作用在q*k的结果上的,q*k的shape是(bs,head_num,seqlen,seqlen)的矩阵，所以mask也是这个形状\n",
    "        mask=torch.full((1,1,max_seqlen,max_seqlen),float(-1e9))\n",
    "        mask=torch.tril(mask, diagonal=0)\n",
    "        self.register_buffer(\"mask\", mask, persistent=False)\n",
    "\n",
    "    def forward(self,x,\n",
    "                pos_cis=None,\n",
    "                past_key_value=None,\n",
    "                use_cache=False):\n",
    "        #输入x,pos_cis用来进行旋转编码处理\n",
    "        #past_key_value用来进行缓存处理，如果use_cache=True，则past_key_value会被使用\n",
    "        #返回值为attn_output, attn_weights\n",
    "        #### 获取x的形状信息 ####\n",
    "        bs,seqlen,_=x.shape\n",
    "        #### 进行分头处理 ####\n",
    "        xq=self.q_proj(x).view(bs,seqlen,self.head_num,self.head_dim)\n",
    "        xk=self.k_proj(x).view(bs,seqlen,self.kv_head_num,self.head_dim)\n",
    "        xk=repeat_kv_heads(xk,self.rep_num)\n",
    "        xv=self.v_proj(x).view(bs,seqlen,self.kv_head_num,self.head_dim)\n",
    "        xv=repeat_kv_heads(xv,self.rep_num)\n",
    "        #### RoPE处理 ####\n",
    "        xq,xk=apply_rotary_emb(xq,xk,pos_cis)\n",
    "        #### KV_cache处理(仅在推理模式可用) ####\n",
    "        if past_key_value is not None:\n",
    "            #past_key_value的形状是(bs, seqlen, kv_head_num, head_dim)\n",
    "            #将past_key_value的形状调整为(bs, seqlen, kv_head_num, head_dim)\n",
    "            xk=torch.cat([past_key_value[0], xk], dim=1)\n",
    "            xv=torch.cat([past_key_value[1], xv], dim=1)\n",
    "        past_kv=(xk,xv) if use_cache else None\n",
    "        #### qkv形状调整，进行计算 #### \n",
    "        xq,xk,xv=(\n",
    "            xq.transpose(1, 2),\n",
    "            xk.transpose(1, 2),\n",
    "            xv.transpose(1, 2)\n",
    "        )\n",
    "        #### 计算注意力权重 ####\n",
    "        if self.Flash and seqlen!=1:\n",
    "            # 在使用Flash Attention且计算序列长度不为1时，使用Flash Attention，seqlen!=1是因为seqlen=1时，Flash Attention会报错\n",
    "            dropout_p=self.dropout if self.training else 0.0\n",
    "            output=F.scaled_dot_product_attention(\n",
    "                xq,xk,xv,\n",
    "                attn_mask=None,#这里的attn_mask是None，因为下面设置了is_causal=True,所以会自动应用因果下三角掩码，这是flash_attn的机制决定的\n",
    "                dropout_p=dropout_p,\n",
    "                is_causal=True\n",
    "            )\n",
    "        else:\n",
    "            #不使用flash attention时，使用传统的注意力计算方式\n",
    "            scores=torch.matmul(xq,xk.transpose(-2,-1))/math.sqrt(self.head_dim)\n",
    "            scores+=self.mask[:,:,:seqlen,:seqlen]\n",
    "            attn_weights=F.softmax(scores,dim=-1)\n",
    "            attn_weights=self.attn_dropout(attn_weights)\n",
    "            output=torch.matmul(attn_weights,xv)\n",
    "        #### 整合output ####\n",
    "        output=output.transpose(1,2).reshape(bs,seqlen,-1)\n",
    "        output=self.res_dropout(self.o_proj(output))\n",
    "        return output,past_kv\n",
    "\n",
    "\n",
    "# 修正后的测试程序\n",
    "input_ids = torch.rand(2, 3, 128)  # 批次大小=2, 序列长度=3, 嵌入维度=64\n",
    "# 修正：传递正确的vocab_size（假设为1000），embed_dim=64, head_num=8, kv_head_num=4\n",
    "GQALayer = GroupQueryAttention(embed_dim=128, head_num=8, kv_head_num=4)\n",
    "# 创建随机的pos_cis用于测试\n",
    "pos_cis = precompute_pos_cis(16,3)\n",
    "output, _ = GQALayer(input_ids, pos_cis=pos_cis) \n",
    "print(\"output shape:\", output.shape)\n",
    "print(\"output:\", output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc5c2c93",
   "metadata": {},
   "source": [
    "## RMSNorm\n",
    "### 均方根层归一化 (Root Mean Square Layer Normalization, RMSNorm)\n",
    "\n",
    "RMSNorm 是对 LayerNorm 的一个改进,  没有做 re-center 操作（移除了均值项）, 可以看作 LayerNorm 在均值为零时的特例, 使用平方根均值归一化降低噪声影响。\n",
    "\n",
    "- **Layer Norm**\n",
    "\n",
    "$$y = \\frac{x-E(x)}{\\sqrt{Var(x) + \\epsilon}} * \\gamma + \\beta$$\n",
    "\n",
    "假设输入张量形状为 (batch_size,  sequence_length,  embedding_dim), 层归一化对 embedding_dim 维度进行归一化操作, 其中,  $\\epsilon$ 是一个超参数, 用于防止分母为零导致结果上溢,  $\\gamma$,  $\\beta$ 均为可学习参数。\n",
    "\n",
    "- **RMS Norm**\n",
    "\n",
    "$$a_i=\\frac{a_i}{RMS(a) + \\epsilon} * \\gamma,  \\quad where \\quad RMS(a) = \\sqrt{\\frac{1}{n}\\sum^n_{i=1}a^2_i}.$$\n",
    "\n",
    "假设输入张量形状为 (batch_size,  sequence_length,  embedding_dim), RMS Norm 对 embedding_dim 维度进行归一化,其中,  其中,  $\\epsilon$ 是一个超参数, 用于防止分母为零导致结果上溢, $\\gamma$ 为可学习参数.\n",
    "\n",
    "不难发现, 当均值为零时, Layer Norm 退化为 RMS Norm. 这是因为 RMS Norm 在 Layer Norm 的基础上舍弃了中心化操作, 仅用缩放进行归一化, 其不改变数据原本的分布, 有利于激活函数输出的稳定."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0a5134f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Tensor:\n",
      " tensor([[[ 1.2416, -0.9110, -2.1397],\n",
      "         [-0.4401, -0.0174,  1.5917]],\n",
      "\n",
      "        [[-0.4660,  0.8371, -1.8996],\n",
      "         [ 0.3148, -0.7884,  0.8145]]])\n",
      "Output Tensor:\n",
      " tensor([[[ 0.8157, -0.5985, -1.4058],\n",
      "         [-0.4615, -0.0183,  1.6693]],\n",
      "\n",
      "        [[-0.3794,  0.6815, -1.5465],\n",
      "         [ 0.4635, -1.1607,  1.1991]]], grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "class RMSNorm(nn.Module):\n",
    "    def __init__(self,dim,eps=1e-9):\n",
    "        super(RMSNorm,self).__init__()\n",
    "        self.eps=eps\n",
    "        self.weight=nn.Parameter(torch.ones(dim))\n",
    "    def forward(self,x):\n",
    "        return self.weight*x*torch.rsqrt(x.pow(2).mean(-1,keepdim=True)+self.eps)\n",
    "#测试RMSNorm\n",
    "x=torch.randn(2,2,3)\n",
    "rmslayer=RMSNorm(3,1e-7)\n",
    "output=rmslayer(x)\n",
    "print(\"Input Tensor:\\n\", x)\n",
    "print(\"Output Tensor:\\n\", output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba843ddf",
   "metadata": {},
   "source": [
    "## FFN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6c07d099",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Tensor:\n",
      " tensor([[[ 1.5002,  2.4258, -0.3180,  0.5998],\n",
      "         [ 2.0557, -0.1931, -1.3039, -0.5660],\n",
      "         [-0.3713,  0.5173, -0.1114, -1.2280]],\n",
      "\n",
      "        [[ 0.3649,  0.3621, -0.5287,  1.4578],\n",
      "         [-0.4595,  0.4389,  0.2427, -0.3213],\n",
      "         [-1.0624, -0.6268, -0.3982,  1.1072]]])\n",
      "Output Tensor:\n",
      " tensor([[[ 1.0263, -0.5621, -0.5764,  0.2843],\n",
      "         [ 0.3159,  0.0451, -1.1959, -0.2367],\n",
      "         [ 0.5443,  0.1495, -0.3052, -0.4412]],\n",
      "\n",
      "        [[ 0.6623, -0.6118, -0.5171,  0.2873],\n",
      "         [ 0.7190, -0.2367, -0.1903, -0.1967],\n",
      "         [ 0.5744, -0.4669,  0.0507,  0.2224]]], grad_fn=<ViewBackward0>)\n",
      "Output Tensor Shape: torch.Size([2, 3, 4])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self,embed_dim,hidden_dim,dropout=0.1):\n",
    "        super(FeedForward,self).__init__()\n",
    "        self.up_proj=nn.Linear(embed_dim,hidden_dim)\n",
    "        self.gate_proj=nn.Linear(embed_dim,hidden_dim)\n",
    "        self.down_proj=nn.Linear(hidden_dim,embed_dim)\n",
    "        self.dropout=nn.Dropout(dropout)\n",
    "    def forward(self,x):\n",
    "        residual=F.silu(self.gate_proj(x))\n",
    "        residual=self.dropout(residual)\n",
    "        x=self.up_proj(x)\n",
    "        x+=residual\n",
    "        x=self.down_proj(x)\n",
    "        return x\n",
    "\n",
    "bs, seqlen, embed_dim = 2, 3, 4\n",
    "ffn_dim = 8\n",
    "x = torch.randn(bs, seqlen, embed_dim)\n",
    "ffn_layer = FeedForward(embed_dim, ffn_dim)\n",
    "output = ffn_layer(x)\n",
    "print(\"Input Tensor:\\n\", x)\n",
    "print(\"Output Tensor:\\n\", output)\n",
    "#测试FeedForward的输出形状\n",
    "print(\"Output Tensor Shape:\", output.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2debd01c",
   "metadata": {},
   "source": [
    "# 组成MinimindBlock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ac736bf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids.shape= torch.Size([2, 64, 128])\n",
      "head_dim= 8\n",
      "pos_cis.shape torch.Size([64, 4])\n",
      "xq_,shape torch.Size([2, 64, 16, 4])\n",
      "xk_,shape torch.Size([2, 64, 16, 4])\n",
      "xq_.shape= torch.Size([2, 64, 16, 4])\n",
      "pos_cis shape: torch.Size([1, 64, 1, 4])\n",
      "xq_ shape: torch.Size([2, 64, 16, 4])\n",
      "xk_ shape: torch.Size([2, 64, 16, 4])\n",
      "torch.Size([2, 64, 128])\n"
     ]
    }
   ],
   "source": [
    "class MinimindBlock(nn.Module):\n",
    "    def __init__(self,layer_id,seqlen,embed_dim,head_num,kv_head_num,hidden_dim):\n",
    "        super(MinimindBlock,self).__init__()\n",
    "        self.layer_id=layer_id\n",
    "        self.seqlen=seqlen\n",
    "        self.embed_dim=embed_dim\n",
    "        self.head_num=head_num\n",
    "        self.kv_head_num=kv_head_num\n",
    "        self.hidden_dim=hidden_dim\n",
    "        self.head_dim=self.embed_dim//self.head_num\n",
    "        \n",
    "        assert self.embed_dim==self.head_num*self.head_dim,\"head_num must be integer\"\n",
    "        #### 组件初始化\n",
    "        self.norm1=RMSNorm(self.embed_dim)\n",
    "        self.norm2=RMSNorm(self.embed_dim)\n",
    "        self.attn1=GroupQueryAttention(self.embed_dim,self.head_num,self.kv_head_num)\n",
    "        self.ffn=FeedForward(self.embed_dim,self.hidden_dim)\n",
    "        #### 额外的初始化\n",
    "        print(\"head_dim=\",self.head_dim)\n",
    "        pos_cis=precompute_pos_cis(self.head_dim,self.seqlen)\n",
    "        self.register_buffer(\"pos_cis\",pos_cis)\n",
    "        print(\"pos_cis.shape\",pos_cis.shape)\n",
    "\n",
    "    def forward(self,x):\n",
    "        x=self.norm1(x)\n",
    "        x,_=self.attn1(x,self.pos_cis)\n",
    "        x=self.norm2(x)\n",
    "        x=self.ffn(x)\n",
    "        return x\n",
    "\n",
    "bs,seqlen,embed_dim=2,64,128\n",
    "head_num=16\n",
    "kv_head_num=4\n",
    "hidden_dim=1024\n",
    "input_ids=torch.randn(bs,seqlen,embed_dim)\n",
    "print(\"input_ids.shape=\",input_ids.shape)\n",
    "block1=MinimindBlock(1,seqlen,embed_dim,head_num,kv_head_num,hidden_dim)\n",
    "res=block1(input_ids)\n",
    "print(res.shape)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c11d1fdf",
   "metadata": {},
   "source": [
    "# MiniMindLM Dense\n",
    "回顾结构<br>\n",
    "![img](../images/LLM-structure.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ffac1504",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "head_dim= 4\n",
      "pos_cis.shape torch.Size([32, 2])\n",
      "head_dim= 4\n",
      "pos_cis.shape torch.Size([32, 2])\n",
      "xq_,shape torch.Size([2, 32, 16, 2])\n",
      "xk_,shape torch.Size([2, 32, 16, 2])\n",
      "xq_.shape= torch.Size([2, 32, 16, 2])\n",
      "pos_cis shape: torch.Size([1, 32, 1, 2])\n",
      "xq_ shape: torch.Size([2, 32, 16, 2])\n",
      "xk_ shape: torch.Size([2, 32, 16, 2])\n",
      "xq_,shape torch.Size([2, 32, 16, 2])\n",
      "xk_,shape torch.Size([2, 32, 16, 2])\n",
      "xq_.shape= torch.Size([2, 32, 16, 2])\n",
      "pos_cis shape: torch.Size([1, 32, 1, 2])\n",
      "xq_ shape: torch.Size([2, 32, 16, 2])\n",
      "xk_ shape: torch.Size([2, 32, 16, 2])\n",
      "input_ids.shape= torch.Size([2, 32])\n",
      "input_ids= tensor([[ 512,  565,  963,  532,  573,  872,  708,  606,  501,  928,  761,  671,\n",
      "          713,  422,  354,  466,  335,  128,  363,  451,  600,  882,    5,  744,\n",
      "          532,  139,  112,  937,  444,    3,  896,  258],\n",
      "        [ 961,  552,  368,  464,  521,  134,  546,  799,  630,  232,  176,  152,\n",
      "          730,  389,  392,   78,  102,  338,  985,  479,  832,  679,  847,  936,\n",
      "          245,  333,  274,  127,  644,  459,  383, 1017]])\n",
      "res.shape= torch.Size([2, 32, 1024])\n",
      "res= tensor([[[0.0008, 0.0009, 0.0006,  ..., 0.0027, 0.0004, 0.0009],\n",
      "         [0.0011, 0.0015, 0.0007,  ..., 0.0025, 0.0004, 0.0010],\n",
      "         [0.0008, 0.0013, 0.0006,  ..., 0.0023, 0.0004, 0.0011],\n",
      "         ...,\n",
      "         [0.0006, 0.0010, 0.0005,  ..., 0.0010, 0.0004, 0.0009],\n",
      "         [0.0014, 0.0008, 0.0006,  ..., 0.0030, 0.0004, 0.0006],\n",
      "         [0.0011, 0.0015, 0.0005,  ..., 0.0030, 0.0003, 0.0012]],\n",
      "\n",
      "        [[0.0018, 0.0031, 0.0007,  ..., 0.0016, 0.0007, 0.0010],\n",
      "         [0.0023, 0.0020, 0.0007,  ..., 0.0018, 0.0007, 0.0013],\n",
      "         [0.0017, 0.0021, 0.0009,  ..., 0.0020, 0.0007, 0.0011],\n",
      "         ...,\n",
      "         [0.0019, 0.0013, 0.0006,  ..., 0.0019, 0.0005, 0.0011],\n",
      "         [0.0016, 0.0018, 0.0006,  ..., 0.0022, 0.0007, 0.0016],\n",
      "         [0.0018, 0.0025, 0.0010,  ..., 0.0014, 0.0006, 0.0011]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "class MinimindLM(nn.Module):\n",
    "    def __init__(self,blocknum,vocab_size,seqlen,embed_dim,head_num,kv_head_num,hidden_dim):\n",
    "        super(MinimindLM,self).__init__()\n",
    "        #### 属性继承 ####\n",
    "        self.blocknum=blocknum\n",
    "        self.vocab_size=vocab_size\n",
    "        self.seqlen=seqlen\n",
    "        self.embed_dim=embed_dim\n",
    "        self.head_num=head_num\n",
    "        self.kv_head_num=kv_head_num\n",
    "        self.hidden_num=hidden_dim\n",
    "        self.params=(seqlen,embed_dim,head_num,kv_head_num,hidden_dim)\n",
    "        #### 定义各个部件 ####\n",
    "        self.embed=nn.Embedding(self.vocab_size,self.embed_dim)\n",
    "        self.blocks=nn.ModuleList([MinimindBlock(i,*(self.params)) for i in range(self.blocknum) ])\n",
    "        self.norm=RMSNorm(self.embed_dim)\n",
    "        self.linear=nn.Linear(self.embed_dim,self.vocab_size)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x=self.embed(x)\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "        x=self.norm(x)\n",
    "        x=self.linear(x)\n",
    "        x=F.softmax(x,dim=-1)\n",
    "        return x\n",
    "\n",
    "blocknum=2\n",
    "vocab_size=1024\n",
    "seqlen=32\n",
    "embed_dim=64\n",
    "head_num=16\n",
    "kv_head_num=4\n",
    "hidden_dim=256\n",
    "params=(blocknum,vocab_size,seqlen,embed_dim,head_num,kv_head_num,hidden_dim)\n",
    "input_ids=torch.randint(0,1024,(2,32))\n",
    "minimindLM=MinimindLM(*params)\n",
    "res=minimindLM(input_ids)\n",
    "print(\"input_ids.shape=\",input_ids.shape)\n",
    "print(\"input_ids=\",input_ids)\n",
    "print(\"res.shape=\",res.shape)\n",
    "print(\"res=\",res)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55f58779",
   "metadata": {},
   "source": [
    "## 目前存在的一些疑问\n",
    "- 输入的input_ids 过长/过短如何处理\n",
    "- padding和mask的具体作用机制\n",
    "- 如何来把tokenizer结合进去\n",
    "- 如何利用transformers库来"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10184190",
   "metadata": {},
   "source": [
    "# 利用transformers库封装这个最原始的模型\n",
    "[关于transformers库的说明](https://cloud.tencent.com/developer/article/2367010)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af1f11f5",
   "metadata": {},
   "source": [
    "generate和forward最大区别在于：generate仅仅用于推理，而forward则训练和推理都能使用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c8af14d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "minimind",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
