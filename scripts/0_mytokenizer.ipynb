{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e7dd4746",
   "metadata": {},
   "source": [
    "# 学习Tokenizer\n",
    "[参考链接](https://zhuanlan.zhihu.com/p/657047389)第6节开始的内容"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "642b3e2b",
   "metadata": {},
   "source": [
    "## Tokenizer的运行机制\n",
    "### 第一步：Normalization\n",
    "这一步是：\n",
    "- removing needless whitespace\n",
    "- lowercasing\n",
    "- removing accents<br>\n",
    "也就是移除空格，大小写重置，removeing_accents就是café Gómez résumé变成cafe Gomez resume\n",
    "\n",
    "### 第二步：Pre-tokenization\n",
    "tokenizer 是不可以直接在原始的文本上训练的，需要做一些处理，比如这里的将句子切分成一个个词汇。这个环节叫做 Pre-tokenization。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d9f35ac3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/zyp/miniconda3/envs/minimind/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tokenizers.Tokenizer'>\n",
      "hello how are u?\n",
      "hello how are u?\n",
      "[('Hello', (0, 5)), (',', (5, 6)), ('world', (7, 12)), ('!', (12, 13)), ('This', (14, 18)), ('is', (19, 21)), ('a', (22, 23)), ('test', (24, 28)), ('.', (28, 29))]\n",
      "[('Hello', (0, 5)), (',', (5, 6)), ('Ġworld', (6, 12)), ('!', (12, 13)), ('ĠThis', (13, 18)), ('Ġis', (18, 21)), ('Ġa', (21, 23)), ('Ġtest', (23, 28)), ('.', (28, 29))]\n",
      "[('▁Hello,', (0, 6)), ('▁world!', (7, 13)), ('▁This', (14, 18)), ('▁is', (19, 21)), ('▁a', (22, 23)), ('▁test.', (24, 29))]\n"
     ]
    }
   ],
   "source": [
    "### pre-tokenization演示\n",
    "from transformers import AutoTokenizer\n",
    "tokenizer= AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "text = \"Hello, world! This is a test.\"\n",
    "text1=\"Héllò hôw are ü?\"\n",
    "text2=\"Héllò Hôw ARE U?\"\n",
    "print(type(tokenizer.backend_tokenizer))\n",
    "#remove accents表示\n",
    "print(tokenizer.backend_tokenizer.normalizer.normalize_str(\"Héllò hôw are ü?\"))\n",
    "#lowercasting\n",
    "print(tokenizer.backend_tokenizer.normalizer.normalize_str(\"Héllò Hôw ARE U?\"))\n",
    "\n",
    "\n",
    "### pretokenize 演示\n",
    "print(tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(text)) #bert的分词处理是左闭右开\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "print(tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(text)) #gpt2的分词处理空格被替换成了 Ġ\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"t5-small\")\n",
    "print(tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(text)) #t5的分词处理空格被替换成了_,并且句首自动添加了_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba5a39eb",
   "metadata": {},
   "source": [
    "### Tokenizer实战-minimind\n",
    "复现minimind采用的BPE-tokenizer<br>\n",
    "由于分词及其重要和依赖准确度，一般用到的都是成熟的bpe，这里的训练仅仅是为了跑通流程，直到其中各种流程和细节。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0f32d9a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import (\n",
    "    Tokenizer,\n",
    "    models,\n",
    "    pre_tokenizers,\n",
    "    normalizers,\n",
    "    processors,\n",
    "    trainers,\n",
    "    decoders,\n",
    ")\n",
    "tokenizer = Tokenizer(models.BPE())\n",
    "tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel(add_prefix_space=False) #不添加前缀空格，和GPT的BPE对齐\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63efdf21",
   "metadata": {},
   "source": [
    "### 记录特殊词汇表\n",
    "除了最基本的word文本，还需要定义一些文本标记的符号，比如开始符号和结束符号\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "df059c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "special_tokens=[\"<|sos|>\", \"<|eos|>\",\"<unk>\",\"<s>\",\"</s>\",\"<pad>\",\"<mask>\"]\n",
    "trainer = trainers.BpeTrainer(\n",
    "    vocab_size=256,\n",
    "    special_tokens=special_tokens,\n",
    "    show_progress=True,\n",
    "    initial_alphabet=pre_tokenizers.ByteLevel.alphabet(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1964cfdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "使用ByteLevel初始字符集的分词结果：\n",
      "['Ġ', 'H', 'e', 'l', 'l', 'o', ',', 'Ġ', 't', 'h', 'i', 's', 'Ġ', 'i', 's', 'Ġ', 'a', 'Ġ', 't', 'e', 's', 't', '.', 'Ġ', 'T', 'o', 'k', 'e', 'n', 'i', 'z', 'a', 't', 'i', 'o', 'n', 'Ġ', 'i', 's', 'Ġ', 'f', 'u', 'n', '!', ',', '[UNK]', 'Ġ', ',', '<', 'c', 'l', 's', '>', ',', '[', 'C', 'L', 'S', ']', ',', '[cls]']\n",
      "\n",
      "使用自定义初始字符集的分词结果：\n",
      "['Ġ', 'H', 'e', 'l', 'l', 'o', ',', 'Ġ', 't', 'h', 'i', 's', 'Ġ', 'i', 's', 'Ġ', 'a', 'Ġ', 't', 'e', 's', 't', '.', 'Ġ', 'T', 'o', 'k', 'e', 'n', 'i', 'z', 'a', 't', 'i', 'o', 'n', 'Ġ', 'i', 's', 'Ġ', 'f', 'u', 'n', '!', ',', '[UNK]', 'Ġ', ',', '<', 'c', 'l', 's', '>', ',', '[CLS]', 'Ġ', ',', '[', 'c', 'l', 's', ']']\n"
     ]
    }
   ],
   "source": [
    "#演示一下show_progress的效果和initial_alphabet的效果\n",
    "from tokenizers import Tokenizer, trainers, pre_tokenizers,models\n",
    "\n",
    "# 准备训练数据\n",
    "data = [\"Hello, this is a test.\", \"Tokenization is fun!,[UNK],<cls>,[CLS],[cls]\"]\n",
    "\n",
    "# 配置1：显示进度，使用ByteLevel初始字符集\n",
    "trainer1 = trainers.BpeTrainer(\n",
    "    vocab_size=8,\n",
    "    special_tokens=[\"[UNK]\", \"[cls]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"],\n",
    "    show_progress=True,  # 显示进度条\n",
    "    initial_alphabet=pre_tokenizers.ByteLevel.alphabet()  # 256字节字符集\n",
    ")\n",
    "tokenizer1 = Tokenizer(models.BPE())\n",
    "tokenizer1.pre_tokenizer = pre_tokenizers.ByteLevel()\n",
    "tokenizer1.train_from_iterator(data, trainer1)\n",
    "\n",
    "# 配置2：不显示进度，自定义初始字符集\n",
    "trainer2 = trainers.BpeTrainer(\n",
    "    vocab_size=8,\n",
    "    special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"],\n",
    "    show_progress=False,  # 不显示进度条\n",
    "    initial_alphabet=['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', \n",
    "                      'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', \n",
    "                      ' ', '!', ',', '.', ':', ';', '?']  # 仅包含常用ASCII字符\n",
    ")\n",
    "tokenizer2 = Tokenizer(models.BPE())\n",
    "tokenizer2.pre_tokenizer = pre_tokenizers.ByteLevel()\n",
    "tokenizer2.train_from_iterator(data, trainer2)\n",
    "\n",
    "# 测试分词结果\n",
    "print(\"\\n使用ByteLevel初始字符集的分词结果：\")\n",
    "# print(tokenizer1.encode(\"Hello, this is a test.你好\").tokens)\n",
    "print(tokenizer1.encode(\"Hello, this is a test.\", \"Tokenization is fun!,[UNK],<cls>,[CLS],[cls]\").tokens)\n",
    "\n",
    "print(\"\\n使用自定义初始字符集的分词结果：\")\n",
    "# print(tokenizer2.encode(\"Hello, this is a test.你好\").tokens)\n",
    "print(tokenizer2.encode(\"Hello, this is a test.\", \"Tokenization is fun!,[UNK],<cls>,[CLS],[cls]\").tokens)\n",
    "#可以看到自定义初始集无法表示中文字符,而且特殊字符是有对应关系的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4c106e3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row 1: <s>近年来，人工智能技术迅速发展，深刻改变了各行各业的面貌。机器学习、自然语言处理、计算机视觉等领域的突破性进展，使得智能产品和服务越来越普及。从智能家居到自动驾驶，再到智能医疗，AI的应用场景正在快速拓展。随着技术的不断进步，未来的人工智能将更加智能、更加贴近人类生活。</s>\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "def read_texts_from_jsonl(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            data = json.loads(line)\n",
    "            yield data['text']\n",
    "\n",
    "data_path = '../../toydata/tokenizer_data.jsonl'\n",
    "data_iter = read_texts_from_jsonl(data_path)\n",
    "print(f'Row 1: {next(data_iter)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "81f544ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer.train_from_iterator(data_iter, trainer=trainer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "865300aa",
   "metadata": {},
   "source": [
    "# 设置decode项\n",
    "decode用于将bpe分词和input_ids的编号对应起来\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "28db1ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decoder = decoders.ByteLevel()\n",
    "assert tokenizer.token_to_id('<unk>') == 2\n",
    "assert tokenizer.token_to_id('<s>') == 3\n",
    "assert tokenizer.token_to_id('</s>') == 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edd52b0b",
   "metadata": {},
   "source": [
    "# 保存训练好的tokenzier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "03a67b06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./model/toy_tokenizer/vocab.json', './model/toy_tokenizer/merges.txt']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "tokenizer_dir = \"./model/toy_tokenizer\"\n",
    "os.makedirs(tokenizer_dir, exist_ok=True)\n",
    "tokenizer.save(os.path.join(tokenizer_dir, \"tokenizer.json\")) # At this point, you will see a file named tokenizer.json under tokenizer_dir\n",
    "tokenizer.model.save(tokenizer_dir) # generate vocab.json & merges.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0e8a6ff",
   "metadata": {},
   "source": [
    "# 手动创建一份配置文件\n",
    "[关于配置文件](https://blog.csdn.net/xiezhipu/article/details/145585777)<br>\n",
    "[关于normalizer的配置说明](https://blog.csdn.net/weixin_49346755/article/details/126496833)<br>\n",
    "[关于post_process的配置说明](https://blog.csdn.net/weixin_49346755/article/details/126499720)<br>\n",
    "[chat_template的设计规则](https://www.guyuehome.com/detail?id=1888166611628642305)<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6769c537",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer training completed and saved.\n"
     ]
    }
   ],
   "source": [
    "config = {\n",
    "    \"add_bos_token\": False,\n",
    "    \"add_eos_token\": False,\n",
    "    \"add_prefix_space\": False,\n",
    "    \"added_tokens_decoder\": {\n",
    "        \"0\":{\n",
    "      \"content\": \"<|sos|>\",\n",
    "      \"single_word\": False,\n",
    "      \"lstrip\": False,\n",
    "      \"rstrip\": False,\n",
    "      \"normalized\": False,\n",
    "      \"special\": True\n",
    "    },\n",
    "    \"1\":{\n",
    "      \"content\": \"<|eos|>\",\n",
    "      \"single_word\": False,\n",
    "      \"lstrip\": False,\n",
    "      \"rstrip\": False,\n",
    "      \"normalized\": False,\n",
    "      \"special\": True\n",
    "    },\n",
    "    \"2\":{\n",
    "      \"content\": \"<unk>\",\n",
    "      \"single_word\": False,\n",
    "      \"lstrip\": False,\n",
    "      \"rstrip\": False,\n",
    "      \"normalized\": False,\n",
    "      \"special\": True\n",
    "    },\n",
    "    \"3\":{\n",
    "      \"content\": \"<s>\",\n",
    "      \"single_word\": False,\n",
    "      \"lstrip\": False,\n",
    "      \"rstrip\": False,\n",
    "      \"normalized\": False,\n",
    "      \"special\": True\n",
    "    },\n",
    "    \"4\":{\n",
    "      \"content\": \"</s>\",\n",
    "      \"single_word\": False,\n",
    "      \"lstrip\": False,\n",
    "      \"rstrip\": False,\n",
    "      \"normalized\": False,\n",
    "      \"special\": True\n",
    "    },\n",
    "    \"5\":{\n",
    "      \"content\": \"<pad>\",\n",
    "      \"single_word\": False,\n",
    "      \"lstrip\": False,\n",
    "      \"rstrip\": False,\n",
    "      \"normalized\": False,\n",
    "      \"special\": True\n",
    "    },\n",
    "    \"6\":{\n",
    "      \"content\": \"<mask>\",\n",
    "      \"single_word\": False,\n",
    "      \"lstrip\": False,\n",
    "      \"rstrip\": False,\n",
    "      \"normalized\": False,\n",
    "      \"special\": True\n",
    "    }\n",
    "    },\n",
    "    \"additional_special_tokens\": [\"mask\",\"<old>\",\"<me>\"],\n",
    "    \"bos_token\": \"<s>\",\n",
    "    \"clean_up_tokenization_spaces\": False,\n",
    "    \"eos_token\": \"</s>\",\n",
    "    \"legacy\": True,\n",
    "    \"model_max_length\": 32768,\n",
    "    \"pad_token\": \"<unk>\",\n",
    "    \"sp_model_kwargs\": {},\n",
    "    \"spaces_between_special_tokens\": False,\n",
    "    \"tokenizer_class\": \"PreTrainedTokenizerFast\",\n",
    "    \"unk_token\": \"<unk>\",\n",
    "    # \"chat_template\": \"{{ '<s>' + messages[0]['text'] + '</s>' }}\"   #普通的模板，只能处理单条数据\n",
    "    \"chat_template\": \"{% for message in messages %} {{ '<s>' + message['text'] + '</s>'}} {% endfor %}\"  #自定义模板，Jinja2模板引擎,可以处理多条对话\n",
    "}\n",
    "\n",
    "with open(os.path.join(tokenizer_dir, \"tokenizer_config.json\"), \"w\", encoding=\"utf-8\") as config_file:\n",
    "    json.dump(config, config_file, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(\"Tokenizer training completed and saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d9c8823",
   "metadata": {},
   "source": [
    "# tokenizer的加载"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bae96866",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "原始文本：[{'text': '<|sos|><mask>this is a test <me><old><pad><|eos|>'}, {'text': '<s> 这是我的测试文本。This is<pad> a test<mask>.</s>'}]\n",
      "修改文本： <s><|sos|><mask>this is a test <me><old><pad><|eos|></s>  <s><s> 这是我的测试文本。This is<pad> a test<mask>.</s></s>  (添加自定义聊天模板)\n",
      "修改文本：[227, 3, 0, 6, 90, 78, 79, 89, 227, 79, 89, 227, 71, 227, 90, 75, 89, 90, 227, 265, 264, 5, 1, 4, 227, 227, 3, 3, 227, 171, 130, 254, 169, 253, 114, 169, 237, 246, 170, 255, 233, 169, 120, 240, 171, 114, 250, 169, 251, 236, 169, 257, 112, 166, 229, 231, 58, 78, 79, 89, 227, 79, 89, 5, 227, 71, 227, 90, 75, 89, 90, 6, 20, 4, 4, 227] (不添加自定义聊天模板)\n"
     ]
    }
   ],
   "source": [
    "from numpy import add\n",
    "from transformers import AutoTokenizer\n",
    "tokenizer_dir1 = \"./model/toy_tokenizer\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_dir1)\n",
    "msg=[{\"text\":\"<|sos|><mask>this is a test <me><old><pad><|eos|>\"},\n",
    "     {\"text\":\"<s> 这是我的测试文本。This is<pad> a test<mask>.</s>\"}\n",
    "]\n",
    "new_msg1 = tokenizer.apply_chat_template(\n",
    "    msg,\n",
    "    tokenize=False,  # 设置为True以进行分词\n",
    "    add_special_tokens=True,  # 添加自定义聊天模板\n",
    ")\n",
    "new_msg2 = tokenizer.apply_chat_template(\n",
    "    msg,\n",
    "    tokenize=True,  # 设置为False以不进行分词\n",
    "    add_special_tokens=False,  # 不添加自定义聊天模板\n",
    ")\n",
    "print(f'原始文本：{msg}')\n",
    "print(f'修改文本：{new_msg1} (添加自定义聊天模板)')\n",
    "print(f'修改文本：{new_msg2} (不添加自定义聊天模板)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "436c78f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "分词器词表大小：263\n"
     ]
    }
   ],
   "source": [
    "print(f'分词器词表大小：{tokenizer.vocab_size}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c0c29fad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "查看分词结果：\n",
      "{'input_ids': [227, 3, 0, 6, 90, 78, 79, 89, 227, 79, 89, 227, 71, 227, 90, 75, 89, 90, 227, 265, 264, 5, 1, 4, 227, 227, 3, 3, 227, 171, 130, 254, 169, 253, 114, 169, 237, 246, 170, 255, 233, 169, 120, 240, 171, 114, 250, 169, 251, 236, 169, 257, 112, 166, 229, 231, 58, 78, 79, 89, 227, 79, 89, 5, 227, 71, 227, 90, 75, 89, 90, 6, 20, 4, 4, 227], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "model_inputs = tokenizer(new_msg1)\n",
    "print(f'查看分词结果：\\n{model_inputs}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1df715bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "对分词结果进行解码： this is a test    这是我的测试文本。This is a test.  (不保留特殊字符)\n"
     ]
    }
   ],
   "source": [
    "response = tokenizer.decode(model_inputs['input_ids'], skip_special_tokens=True)\n",
    "print(f'对分词结果进行解码：{response} (不保留特殊字符)' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66aa03b7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "minimind",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
